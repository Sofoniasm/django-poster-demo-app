
==> Audit <==
|-----------|-------------------------------------|----------|----------------------|---------|---------------------|---------------------|
|  Command  |                Args                 | Profile  |         User         | Version |     Start Time      |      End Time       |
|-----------|-------------------------------------|----------|----------------------|---------|---------------------|---------------------|
| start     |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 10:43 EAT | 29 Feb 24 10:47 EAT |
| start     | --docker-env=DOCKER_CONTEXT=default | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 10:50 EAT | 29 Feb 24 10:50 EAT |
| stop      |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 10:50 EAT | 29 Feb 24 10:51 EAT |
| start     |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 10:51 EAT | 29 Feb 24 10:51 EAT |
| start     |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 11:45 EAT | 29 Feb 24 11:45 EAT |
| stop      |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 11:48 EAT | 29 Feb 24 11:48 EAT |
| start     |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 11:48 EAT | 29 Feb 24 11:48 EAT |
| start     | --nodes=3                           | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 12:16 EAT | 29 Feb 24 12:16 EAT |
| node      | add                                 | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 12:17 EAT | 29 Feb 24 12:17 EAT |
| node      | add                                 | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 12:18 EAT | 29 Feb 24 12:18 EAT |
| dashboard |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 12:20 EAT |                     |
| dashboard |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 12:26 EAT |                     |
| image     | load django-app:latest              | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:15 EAT |                     |
| image     | load django-app:latest              | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:16 EAT |                     |
| image     | load django-app                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:29 EAT |                     |
| image     | load nginx                          | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:30 EAT |                     |
| image     | load django-app:latest              | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:37 EAT |                     |
| stop      |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:38 EAT | 29 Feb 24 15:38 EAT |
| start     |                                     | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:39 EAT | 29 Feb 24 15:39 EAT |
| image     | load django-app:latest              | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:40 EAT |                     |
| image     | load django-app:latest              | minikube | DESKTOP-7DBEA6J\user | v1.32.0 | 29 Feb 24 15:41 EAT |                     |
|-----------|-------------------------------------|----------|----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/02/29 15:39:08
Running on machine: DESKTOP-7DBEA6J
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0229 15:39:08.798148   26856 out.go:296] Setting OutFile to fd 96 ...
I0229 15:39:08.799155   26856 out.go:309] Setting ErrFile to fd 100...
W0229 15:39:08.808239   26856 root.go:314] Error reading config file at C:\Users\user\.minikube\config\config.json: open C:\Users\user\.minikube\config\config.json: The system cannot find the file specified.
I0229 15:39:08.812281   26856 out.go:303] Setting JSON to false
I0229 15:39:08.819233   26856 start.go:128] hostinfo: {"hostname":"DESKTOP-7DBEA6J","uptime":14211,"bootTime":1709196137,"procs":362,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.3155 Build 22631.3155","kernelVersion":"10.0.22631.3155 Build 22631.3155","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"dc19b815-253e-4c99-a3b4-9219833c808c"}
W0229 15:39:08.819233   26856 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0229 15:39:08.819740   26856 out.go:177] 😄  minikube v1.32.0 on Microsoft Windows 11 Pro 10.0.22631.3155 Build 22631.3155
I0229 15:39:08.820261   26856 notify.go:220] Checking for updates...
I0229 15:39:08.820780   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:08.820780   26856 driver.go:378] Setting default libvirt URI to qemu:///system
I0229 15:39:08.920710   26856 docker.go:122] docker version: linux-25.0.3:Docker Desktop 4.27.2 (137060)
I0229 15:39:08.921918   26856 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0229 15:39:09.654919   26856 info.go:266] docker info: {ID:b82e88c8-8092-4f83-b825-3b50c6bc8612 Containers:6 ContainersRunning:2 ContainersPaused:0 ContainersStopped:4 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:72 OomKillDisable:true NGoroutines:109 SystemTime:2024-02-29 12:39:09.612296888 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16626552832 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.4.1]] Warnings:<nil>}}
I0229 15:39:09.656540   26856 out.go:177] ✨  Using the docker driver based on existing profile
I0229 15:39:09.657055   26856 start.go:298] selected driver: docker
I0229 15:39:09.657055   26856 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0229 15:39:09.657055   26856 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0229 15:39:09.658676   26856 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0229 15:39:09.857393   26856 info.go:266] docker info: {ID:b82e88c8-8092-4f83-b825-3b50c6bc8612 Containers:6 ContainersRunning:2 ContainersPaused:0 ContainersStopped:4 Images:6 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:72 OomKillDisable:true NGoroutines:109 SystemTime:2024-02-29 12:39:09.813954758 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:13 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:16626552832 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1-desktop.4] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.5-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container. Vendor:Docker Inc. Version:0.0.24] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.0.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.4.1]] Warnings:<nil>}}
I0229 15:39:09.903698   26856 cni.go:84] Creating CNI manager for ""
I0229 15:39:09.903698   26856 cni.go:136] 3 nodes found, recommending kindnet
I0229 15:39:09.903698   26856 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0229 15:39:09.904748   26856 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0229 15:39:09.904920   26856 cache.go:121] Beginning downloading kic base image for docker with docker
I0229 15:39:09.905435   26856 out.go:177] 🚜  Pulling base image ...
I0229 15:39:09.905962   26856 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0229 15:39:09.905962   26856 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0229 15:39:09.905962   26856 preload.go:148] Found local preload: C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0229 15:39:09.905962   26856 cache.go:56] Caching tarball of preloaded images
I0229 15:39:09.905962   26856 preload.go:174] Found C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0229 15:39:09.905962   26856 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0229 15:39:09.905962   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:10.013410   26856 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0229 15:39:10.013410   26856 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0229 15:39:10.013410   26856 cache.go:194] Successfully downloaded all kic artifacts
I0229 15:39:10.013623   26856 start.go:365] acquiring machines lock for minikube: {Name:mkb3e405ffb35ff57f19f1090207beea089db946 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0229 15:39:10.013738   26856 start.go:369] acquired machines lock for "minikube" in 115.2µs
I0229 15:39:10.013842   26856 start.go:96] Skipping create...Using existing machine configuration
I0229 15:39:10.013842   26856 fix.go:54] fixHost starting: 
I0229 15:39:10.015465   26856 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0229 15:39:10.121844   26856 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0229 15:39:10.121844   26856 fix.go:128] unexpected machine state, will restart: <nil>
I0229 15:39:10.125047   26856 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0229 15:39:10.127152   26856 cli_runner.go:164] Run: docker start minikube
I0229 15:39:10.497665   26856 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0229 15:39:10.604700   26856 kic.go:430] container "minikube" state is running.
I0229 15:39:10.606893   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0229 15:39:10.712614   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:10.713646   26856 machine.go:88] provisioning docker machine ...
I0229 15:39:10.713646   26856 ubuntu.go:169] provisioning hostname "minikube"
I0229 15:39:10.715287   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:10.838001   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:10.838001   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63673 <nil> <nil>}
I0229 15:39:10.838001   26856 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0229 15:39:10.968118   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0229 15:39:10.969181   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:11.071056   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:11.071558   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63673 <nil> <nil>}
I0229 15:39:11.071558   26856 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0229 15:39:11.191551   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0229 15:39:11.191551   26856 ubuntu.go:175] set auth options {CertDir:C:\Users\user\.minikube CaCertPath:C:\Users\user\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\user\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\user\.minikube\machines\server.pem ServerKeyPath:C:\Users\user\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\user\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\user\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\user\.minikube}
I0229 15:39:11.191551   26856 ubuntu.go:177] setting up certificates
I0229 15:39:11.191551   26856 provision.go:83] configureAuth start
I0229 15:39:11.192597   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0229 15:39:11.287658   26856 provision.go:138] copyHostCerts
I0229 15:39:11.288736   26856 exec_runner.go:144] found C:\Users\user\.minikube/ca.pem, removing ...
I0229 15:39:11.288736   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\ca.pem
I0229 15:39:11.293268   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\ca.pem --> C:\Users\user\.minikube/ca.pem (1074 bytes)
I0229 15:39:11.294882   26856 exec_runner.go:144] found C:\Users\user\.minikube/cert.pem, removing ...
I0229 15:39:11.297958   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\cert.pem
I0229 15:39:11.297958   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\cert.pem --> C:\Users\user\.minikube/cert.pem (1115 bytes)
I0229 15:39:11.301522   26856 exec_runner.go:144] found C:\Users\user\.minikube/key.pem, removing ...
I0229 15:39:11.301522   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\key.pem
I0229 15:39:11.301832   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\key.pem --> C:\Users\user\.minikube/key.pem (1675 bytes)
I0229 15:39:11.301832   26856 provision.go:112] generating server cert: C:\Users\user\.minikube\machines\server.pem ca-key=C:\Users\user\.minikube\certs\ca.pem private-key=C:\Users\user\.minikube\certs\ca-key.pem org=user.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0229 15:39:11.356936   26856 provision.go:172] copyRemoteCerts
I0229 15:39:11.364098   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0229 15:39:11.365104   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:11.473385   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:11.556235   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0229 15:39:11.572490   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server.pem --> /etc/docker/server.pem (1196 bytes)
I0229 15:39:11.586527   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0229 15:39:11.601000   26856 provision.go:86] duration metric: configureAuth took 409.449ms
I0229 15:39:11.601000   26856 ubuntu.go:193] setting minikube options for container-runtime
I0229 15:39:11.601000   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:11.602030   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:11.705818   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:11.706337   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63673 <nil> <nil>}
I0229 15:39:11.706337   26856 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0229 15:39:11.823748   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0229 15:39:11.823748   26856 ubuntu.go:71] root file system type: overlay
I0229 15:39:11.823748   26856 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0229 15:39:11.824771   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:11.925125   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:11.925125   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63673 <nil> <nil>}
I0229 15:39:11.925125   26856 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0229 15:39:12.039034   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0229 15:39:12.040067   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:12.142433   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:12.143134   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63673 <nil> <nil>}
I0229 15:39:12.143134   26856 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0229 15:39:12.267350   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0229 15:39:12.267350   26856 machine.go:91] provisioned docker machine in 1.5537041s
I0229 15:39:12.267350   26856 start.go:300] post-start starting for "minikube" (driver="docker")
I0229 15:39:12.267350   26856 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0229 15:39:12.269469   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0229 15:39:12.270541   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:12.375772   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:12.457303   26856 ssh_runner.go:195] Run: cat /etc/os-release
I0229 15:39:12.459983   26856 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0229 15:39:12.459983   26856 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0229 15:39:12.459983   26856 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0229 15:39:12.459983   26856 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0229 15:39:12.459983   26856 filesync.go:126] Scanning C:\Users\user\.minikube\addons for local assets ...
I0229 15:39:12.460529   26856 filesync.go:126] Scanning C:\Users\user\.minikube\files for local assets ...
I0229 15:39:12.460529   26856 start.go:303] post-start completed in 193.1793ms
I0229 15:39:12.462179   26856 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0229 15:39:12.463285   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:12.561291   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:12.654539   26856 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0229 15:39:12.658284   26856 fix.go:56] fixHost completed within 2.6444414s
I0229 15:39:12.658284   26856 start.go:83] releasing machines lock for "minikube", held for 2.6445455s
I0229 15:39:12.659328   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0229 15:39:12.763475   26856 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0229 15:39:12.764703   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:12.765320   26856 ssh_runner.go:195] Run: cat /version.json
I0229 15:39:12.766576   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:12.871548   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:12.879988   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:14.076890   26856 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.3134148s)
I0229 15:39:14.076890   26856 ssh_runner.go:235] Completed: cat /version.json: (1.3115696s)
I0229 15:39:14.079045   26856 ssh_runner.go:195] Run: systemctl --version
I0229 15:39:14.084373   26856 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0229 15:39:14.089643   26856 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0229 15:39:14.095486   26856 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0229 15:39:14.097092   26856 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0229 15:39:14.102699   26856 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0229 15:39:14.102699   26856 start.go:472] detecting cgroup driver to use...
I0229 15:39:14.102699   26856 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0229 15:39:14.102699   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0229 15:39:14.113820   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0229 15:39:14.121350   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0229 15:39:14.127199   26856 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0229 15:39:14.128763   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0229 15:39:14.137000   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0229 15:39:14.144984   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0229 15:39:14.152376   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0229 15:39:14.160007   26856 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0229 15:39:14.167492   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0229 15:39:14.174957   26856 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0229 15:39:14.181998   26856 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0229 15:39:14.188881   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:14.257474   26856 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0229 15:39:14.343445   26856 start.go:472] detecting cgroup driver to use...
I0229 15:39:14.343445   26856 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0229 15:39:14.345543   26856 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0229 15:39:14.353351   26856 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0229 15:39:14.355450   26856 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0229 15:39:14.363821   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0229 15:39:14.375841   26856 ssh_runner.go:195] Run: which cri-dockerd
I0229 15:39:14.380518   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0229 15:39:14.386804   26856 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0229 15:39:14.412244   26856 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0229 15:39:14.487272   26856 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0229 15:39:14.577126   26856 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0229 15:39:14.577126   26856 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0229 15:39:14.590347   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:14.667144   26856 ssh_runner.go:195] Run: sudo systemctl restart docker
I0229 15:39:14.846921   26856 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0229 15:39:14.918109   26856 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0229 15:39:14.996715   26856 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0229 15:39:15.034795   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:15.117272   26856 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0229 15:39:15.128129   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:15.207451   26856 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0229 15:39:15.261978   26856 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0229 15:39:15.264079   26856 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0229 15:39:15.267243   26856 start.go:540] Will wait 60s for crictl version
I0229 15:39:15.269387   26856 ssh_runner.go:195] Run: which crictl
I0229 15:39:15.273554   26856 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0229 15:39:15.304498   26856 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0229 15:39:15.305016   26856 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0229 15:39:15.322691   26856 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0229 15:39:15.339260   26856 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0229 15:39:15.340315   26856 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0229 15:39:15.487050   26856 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0229 15:39:15.488642   26856 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0229 15:39:15.493098   26856 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0229 15:39:15.501565   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0229 15:39:15.596353   26856 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0229 15:39:15.597431   26856 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0229 15:39:15.613173   26856 docker.go:671] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
kindest/kindnetd:v20230809-80a64d96
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0229 15:39:15.613173   26856 docker.go:601] Images already preloaded, skipping extraction
I0229 15:39:15.614213   26856 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0229 15:39:15.627482   26856 docker.go:671] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
kindest/kindnetd:v20230809-80a64d96
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0229 15:39:15.627482   26856 cache_images.go:84] Images are preloaded, skipping loading
I0229 15:39:15.628505   26856 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0229 15:39:15.666309   26856 cni.go:84] Creating CNI manager for ""
I0229 15:39:15.666309   26856 cni.go:136] 3 nodes found, recommending kindnet
I0229 15:39:15.666309   26856 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0229 15:39:15.666309   26856 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0229 15:39:15.666830   26856 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0229 15:39:15.666830   26856 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0229 15:39:15.668439   26856 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0229 15:39:15.674215   26856 binaries.go:44] Found k8s binaries, skipping transfer
I0229 15:39:15.676308   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0229 15:39:15.681857   26856 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0229 15:39:15.692126   26856 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0229 15:39:15.702330   26856 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0229 15:39:15.713989   26856 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0229 15:39:15.716631   26856 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0229 15:39:15.723039   26856 certs.go:56] Setting up C:\Users\user\.minikube\profiles\minikube for IP: 192.168.49.2
I0229 15:39:15.723039   26856 certs.go:190] acquiring lock for shared ca certs: {Name:mk7164a99e354388efd17e38c9f3aba55425681b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0229 15:39:15.727933   26856 certs.go:199] skipping minikubeCA CA generation: C:\Users\user\.minikube\ca.key
I0229 15:39:15.734500   26856 certs.go:199] skipping proxyClientCA CA generation: C:\Users\user\.minikube\proxy-client-ca.key
I0229 15:39:15.735008   26856 certs.go:315] skipping minikube-user signed cert generation: C:\Users\user\.minikube\profiles\minikube\client.key
I0229 15:39:15.742659   26856 certs.go:315] skipping minikube signed cert generation: C:\Users\user\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I0229 15:39:15.749129   26856 certs.go:315] skipping aggregator signed cert generation: C:\Users\user\.minikube\profiles\minikube\proxy-client.key
I0229 15:39:15.750265   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\ca-key.pem (1675 bytes)
I0229 15:39:15.750265   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\ca.pem (1074 bytes)
I0229 15:39:15.750265   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\cert.pem (1115 bytes)
I0229 15:39:15.750265   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\key.pem (1675 bytes)
I0229 15:39:15.751825   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0229 15:39:15.767078   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0229 15:39:15.780247   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0229 15:39:15.793947   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0229 15:39:15.807779   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0229 15:39:15.820686   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0229 15:39:15.834175   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0229 15:39:15.847365   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0229 15:39:15.862324   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0229 15:39:15.876163   26856 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0229 15:39:15.888465   26856 ssh_runner.go:195] Run: openssl version
I0229 15:39:15.893841   26856 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0229 15:39:15.901969   26856 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:15.904570   26856 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb 29 07:47 /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:15.906706   26856 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:15.913083   26856 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0229 15:39:15.920558   26856 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0229 15:39:15.924986   26856 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0229 15:39:15.931226   26856 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0229 15:39:15.937775   26856 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0229 15:39:15.943820   26856 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0229 15:39:15.951360   26856 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0229 15:39:15.957243   26856 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0229 15:39:15.961774   26856 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0229 15:39:15.962846   26856 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0229 15:39:15.978142   26856 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0229 15:39:15.984034   26856 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0229 15:39:15.984034   26856 kubeadm.go:636] restartCluster start
I0229 15:39:15.985644   26856 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0229 15:39:15.991079   26856 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0229 15:39:15.991618   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0229 15:39:16.091051   26856 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in C:\Users\user\.kube\config
I0229 15:39:16.091051   26856 kubeconfig.go:146] "minikube" context is missing from C:\Users\user\.kube\config - will repair!
I0229 15:39:16.091618   26856 lock.go:35] WriteFile acquiring C:\Users\user\.kube\config: {Name:mkf0568b8c360d314f1ebed2036702ffdf9396af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0229 15:39:16.102761   26856 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0229 15:39:16.110789   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:16.113960   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:16.120840   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:16.120840   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:16.123528   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:16.130117   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:16.639854   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:16.641378   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:16.651656   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:17.145051   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:17.146530   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:17.157192   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:17.633366   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:17.634818   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:17.645799   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:18.137872   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:18.139297   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:18.149905   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:18.641466   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:18.643116   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:18.653072   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:19.130386   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:19.131937   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:19.141816   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:19.635962   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:19.638800   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:19.647885   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:20.139636   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:20.142418   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:20.151418   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:20.643918   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:20.646110   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:20.654189   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:21.131535   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:21.134440   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:21.142918   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:21.634787   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:21.636575   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:21.644447   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:22.137775   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:22.140367   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:22.149313   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:22.641224   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:22.642656   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:22.653469   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:23.145005   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:23.147403   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:23.156159   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:23.633948   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:23.635291   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:23.645387   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:24.141773   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:24.144069   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:24.151400   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:24.645178   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:24.648492   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:24.656717   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:25.140683   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:25.142480   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:25.151227   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:25.645165   26856 api_server.go:166] Checking apiserver status ...
I0229 15:39:25.646811   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0229 15:39:25.655483   26856 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0229 15:39:26.111287   26856 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0229 15:39:26.111287   26856 kubeadm.go:1128] stopping kube-system containers ...
I0229 15:39:26.111961   26856 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0229 15:39:26.128718   26856 docker.go:469] Stopping containers: [06c7c2f6eadc 8cf8442b0d86 dda3a41a93f3 db93da1f747f 7ac9084a2ca9 ffeec317e347 9b30abcf576c 5f6d7711783a 695dde55572e ddfb6032236e cbf6206c2d85 d7a5ac520823 c8edfd8945c4 15572e6e4bf6 fee1ad122a5a 14b8ccee1257 64e7fd8ebf4c 02b19c23d7f5 28ed239f0792 d00bbf5db982 b5f95ec6ce82 36e338eed612 35c16c08ae26 a4b85aca6701 2bba32d76aee 6bd169280e2c 0816f03c7a6c 640c2cee93bc e7061be6543e]
I0229 15:39:26.130275   26856 ssh_runner.go:195] Run: docker stop 06c7c2f6eadc 8cf8442b0d86 dda3a41a93f3 db93da1f747f 7ac9084a2ca9 ffeec317e347 9b30abcf576c 5f6d7711783a 695dde55572e ddfb6032236e cbf6206c2d85 d7a5ac520823 c8edfd8945c4 15572e6e4bf6 fee1ad122a5a 14b8ccee1257 64e7fd8ebf4c 02b19c23d7f5 28ed239f0792 d00bbf5db982 b5f95ec6ce82 36e338eed612 35c16c08ae26 a4b85aca6701 2bba32d76aee 6bd169280e2c 0816f03c7a6c 640c2cee93bc e7061be6543e
I0229 15:39:26.146193   26856 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0229 15:39:26.155711   26856 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0229 15:39:26.161178   26856 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Feb 29 07:47 /etc/kubernetes/admin.conf
-rw------- 1 root root 5652 Feb 29 08:48 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Feb 29 07:47 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Feb 29 08:48 /etc/kubernetes/scheduler.conf

I0229 15:39:26.163321   26856 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0229 15:39:26.170251   26856 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0229 15:39:26.177531   26856 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0229 15:39:26.182817   26856 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0229 15:39:26.184429   26856 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0229 15:39:26.191914   26856 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0229 15:39:26.197190   26856 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0229 15:39:26.199320   26856 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0229 15:39:26.205995   26856 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0229 15:39:26.211856   26856 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0229 15:39:26.211856   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0229 15:39:26.251408   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0229 15:39:26.883856   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0229 15:39:27.013808   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0229 15:39:27.047669   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0229 15:39:27.082116   26856 api_server.go:52] waiting for apiserver process to appear ...
I0229 15:39:27.084178   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0229 15:39:27.118988   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0229 15:39:27.630511   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0229 15:39:28.129316   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0229 15:39:28.137644   26856 api_server.go:72] duration metric: took 1.0555275s to wait for apiserver process to appear ...
I0229 15:39:28.137644   26856 api_server.go:88] waiting for apiserver healthz status ...
I0229 15:39:28.137644   26856 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63672/healthz ...
I0229 15:39:29.836884   26856 api_server.go:279] https://127.0.0.1:63672/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0229 15:39:29.836884   26856 api_server.go:103] status: https://127.0.0.1:63672/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0229 15:39:29.836884   26856 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63672/healthz ...
I0229 15:39:29.924925   26856 api_server.go:279] https://127.0.0.1:63672/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0229 15:39:29.924925   26856 api_server.go:103] status: https://127.0.0.1:63672/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0229 15:39:30.429414   26856 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63672/healthz ...
I0229 15:39:30.434826   26856 api_server.go:279] https://127.0.0.1:63672/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0229 15:39:30.434826   26856 api_server.go:103] status: https://127.0.0.1:63672/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0229 15:39:30.934389   26856 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63672/healthz ...
I0229 15:39:30.938844   26856 api_server.go:279] https://127.0.0.1:63672/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0229 15:39:30.938844   26856 api_server.go:103] status: https://127.0.0.1:63672/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0229 15:39:31.427713   26856 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63672/healthz ...
I0229 15:39:31.433082   26856 api_server.go:279] https://127.0.0.1:63672/healthz returned 200:
ok
I0229 15:39:31.440052   26856 api_server.go:141] control plane version: v1.28.3
I0229 15:39:31.440052   26856 api_server.go:131] duration metric: took 3.3024086s to wait for apiserver health ...
I0229 15:39:31.440052   26856 cni.go:84] Creating CNI manager for ""
I0229 15:39:31.440052   26856 cni.go:136] 3 nodes found, recommending kindnet
I0229 15:39:31.440574   26856 out.go:177] 🔗  Configuring CNI (Container Networking Interface) ...
I0229 15:39:31.444273   26856 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0229 15:39:31.447486   26856 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0229 15:39:31.447486   26856 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0229 15:39:31.459608   26856 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0229 15:39:31.795656   26856 system_pods.go:43] waiting for kube-system pods to appear ...
I0229 15:39:31.802516   26856 system_pods.go:59] 12 kube-system pods found
I0229 15:39:31.802516   26856 system_pods.go:61] "coredns-5dd5756b68-6q95s" [4c4ef054-3860-4f1a-a37f-30084eaf250e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0229 15:39:31.802516   26856 system_pods.go:61] "etcd-minikube" [66edfa1c-88dd-4312-9daa-4ba586b404c4] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0229 15:39:31.802516   26856 system_pods.go:61] "kindnet-2tprc" [6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd] Running
I0229 15:39:31.802516   26856 system_pods.go:61] "kindnet-7jnsb" [dc1a2fb4-c21c-4a95-9c45-a05011b51d78] Running
I0229 15:39:31.802516   26856 system_pods.go:61] "kindnet-9ndrz" [25c2d610-6be0-4dfd-8519-a7bc6e455445] Running
I0229 15:39:31.802516   26856 system_pods.go:61] "kube-apiserver-minikube" [4ff0b3ce-c303-41f9-ae02-a0cc9ee87eea] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0229 15:39:31.802516   26856 system_pods.go:61] "kube-controller-manager-minikube" [1ffdfc26-65a9-4702-9d13-dd1efe895d66] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0229 15:39:31.802516   26856 system_pods.go:61] "kube-proxy-lmbbh" [9693e926-8ec6-4d27-b828-ee61429c0349] Running
I0229 15:39:31.802516   26856 system_pods.go:61] "kube-proxy-qrhwp" [f0d52988-4ba9-4379-a912-25c770957a72] Running
I0229 15:39:31.802516   26856 system_pods.go:61] "kube-proxy-ssxxr" [31ae9eed-ef22-4af7-ad02-33f68fa15032] Running
I0229 15:39:31.802516   26856 system_pods.go:61] "kube-scheduler-minikube" [96041153-c886-4668-b94a-786aa19e3c24] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0229 15:39:31.802516   26856 system_pods.go:61] "storage-provisioner" [a251b6d5-f129-4a41-9eea-19f274e36ba4] Running
I0229 15:39:31.802516   26856 system_pods.go:74] duration metric: took 6.8603ms to wait for pod list to return data ...
I0229 15:39:31.802516   26856 node_conditions.go:102] verifying NodePressure condition ...
I0229 15:39:31.805162   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:31.805162   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:31.805162   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:31.805162   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:31.805162   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:31.805162   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:31.805162   26856 node_conditions.go:105] duration metric: took 2.6463ms to run NodePressure ...
I0229 15:39:31.805162   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0229 15:39:31.903668   26856 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0229 15:39:31.909488   26856 ops.go:34] apiserver oom_adj: -16
I0229 15:39:31.909488   26856 kubeadm.go:640] restartCluster took 15.9254536s
I0229 15:39:31.909488   26856 kubeadm.go:406] StartCluster complete in 15.9477141s
I0229 15:39:31.909488   26856 settings.go:142] acquiring lock: {Name:mkeb1fd696d13c02454084a505541301abc7d400 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0229 15:39:31.909488   26856 settings.go:150] Updating kubeconfig:  C:\Users\user\.kube\config
I0229 15:39:31.910020   26856 lock.go:35] WriteFile acquiring C:\Users\user\.kube\config: {Name:mkf0568b8c360d314f1ebed2036702ffdf9396af Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0229 15:39:31.910541   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0229 15:39:31.910541   26856 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0229 15:39:31.910541   26856 addons.go:69] Setting dashboard=true in profile "minikube"
I0229 15:39:31.910541   26856 addons.go:231] Setting addon dashboard=true in "minikube"
W0229 15:39:31.910541   26856 addons.go:240] addon dashboard should already be in state true
I0229 15:39:31.910541   26856 host.go:66] Checking if "minikube" exists ...
I0229 15:39:31.910541   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:31.912605   26856 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0229 15:39:31.923559   26856 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0229 15:39:31.923559   26856 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0229 15:39:31.924081   26856 out.go:177] 🔎  Verifying Kubernetes components...
I0229 15:39:31.926696   26856 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0229 15:39:31.970685   26856 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0229 15:39:31.972272   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0229 15:39:32.033205   26856 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0229 15:39:32.033730   26856 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0229 15:39:32.034252   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0229 15:39:32.034252   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0229 15:39:32.035296   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:32.078925   26856 api_server.go:52] waiting for apiserver process to appear ...
I0229 15:39:32.081590   26856 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0229 15:39:32.089421   26856 api_server.go:72] duration metric: took 165.8619ms to wait for apiserver process to appear ...
I0229 15:39:32.089421   26856 api_server.go:88] waiting for apiserver healthz status ...
I0229 15:39:32.089421   26856 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63672/healthz ...
I0229 15:39:32.094693   26856 api_server.go:279] https://127.0.0.1:63672/healthz returned 200:
ok
I0229 15:39:32.095737   26856 api_server.go:141] control plane version: v1.28.3
I0229 15:39:32.095737   26856 api_server.go:131] duration metric: took 6.3159ms to wait for apiserver health ...
I0229 15:39:32.095737   26856 system_pods.go:43] waiting for kube-system pods to appear ...
I0229 15:39:32.100986   26856 system_pods.go:59] 12 kube-system pods found
I0229 15:39:32.100986   26856 system_pods.go:61] "coredns-5dd5756b68-6q95s" [4c4ef054-3860-4f1a-a37f-30084eaf250e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0229 15:39:32.100986   26856 system_pods.go:61] "etcd-minikube" [66edfa1c-88dd-4312-9daa-4ba586b404c4] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0229 15:39:32.100986   26856 system_pods.go:61] "kindnet-2tprc" [6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd] Running
I0229 15:39:32.100986   26856 system_pods.go:61] "kindnet-7jnsb" [dc1a2fb4-c21c-4a95-9c45-a05011b51d78] Running
I0229 15:39:32.100986   26856 system_pods.go:61] "kindnet-9ndrz" [25c2d610-6be0-4dfd-8519-a7bc6e455445] Running
I0229 15:39:32.100986   26856 system_pods.go:61] "kube-apiserver-minikube" [4ff0b3ce-c303-41f9-ae02-a0cc9ee87eea] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0229 15:39:32.100986   26856 system_pods.go:61] "kube-controller-manager-minikube" [1ffdfc26-65a9-4702-9d13-dd1efe895d66] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0229 15:39:32.100986   26856 system_pods.go:61] "kube-proxy-lmbbh" [9693e926-8ec6-4d27-b828-ee61429c0349] Running
I0229 15:39:32.100986   26856 system_pods.go:61] "kube-proxy-qrhwp" [f0d52988-4ba9-4379-a912-25c770957a72] Running
I0229 15:39:32.100986   26856 system_pods.go:61] "kube-proxy-ssxxr" [31ae9eed-ef22-4af7-ad02-33f68fa15032] Running
I0229 15:39:32.100986   26856 system_pods.go:61] "kube-scheduler-minikube" [96041153-c886-4668-b94a-786aa19e3c24] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0229 15:39:32.100986   26856 system_pods.go:61] "storage-provisioner" [a251b6d5-f129-4a41-9eea-19f274e36ba4] Running
I0229 15:39:32.100986   26856 system_pods.go:74] duration metric: took 5.2496ms to wait for pod list to return data ...
I0229 15:39:32.100986   26856 kubeadm.go:581] duration metric: took 177.4274ms to wait for : map[apiserver:true system_pods:true] ...
I0229 15:39:32.100986   26856 node_conditions.go:102] verifying NodePressure condition ...
I0229 15:39:32.103596   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:32.103596   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:32.103596   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:32.103596   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:32.103596   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:32.103596   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:32.103596   26856 node_conditions.go:105] duration metric: took 2.6092ms to run NodePressure ...
I0229 15:39:32.103596   26856 start.go:228] waiting for startup goroutines ...
I0229 15:39:32.140946   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:32.240570   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0229 15:39:32.240570   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0229 15:39:32.251271   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0229 15:39:32.251271   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0229 15:39:32.261426   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0229 15:39:32.261426   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0229 15:39:32.270998   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0229 15:39:32.270998   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0229 15:39:32.281274   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0229 15:39:32.281274   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0229 15:39:32.290876   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0229 15:39:32.290876   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0229 15:39:32.301064   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0229 15:39:32.301064   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0229 15:39:32.310620   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0229 15:39:32.310620   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0229 15:39:32.320497   26856 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0229 15:39:32.320497   26856 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0229 15:39:32.332139   26856 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0229 15:39:32.642865   26856 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0229 15:39:32.643398   26856 out.go:177] 🌟  Enabled addons: dashboard
I0229 15:39:32.643923   26856 addons.go:502] enable addons completed in 733.3826ms: enabled=[dashboard]
I0229 15:39:32.643923   26856 start.go:233] waiting for cluster config update ...
I0229 15:39:32.643923   26856 start.go:242] writing updated cluster config ...
I0229 15:39:32.651920   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:32.651920   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:32.653492   26856 out.go:177] 👍  Starting worker node minikube-m02 in cluster minikube
I0229 15:39:32.654012   26856 cache.go:121] Beginning downloading kic base image for docker with docker
I0229 15:39:32.654012   26856 out.go:177] 🚜  Pulling base image ...
I0229 15:39:32.654530   26856 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0229 15:39:32.654530   26856 cache.go:56] Caching tarball of preloaded images
I0229 15:39:32.654530   26856 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0229 15:39:32.655046   26856 preload.go:174] Found C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0229 15:39:32.655046   26856 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0229 15:39:32.655046   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:32.750607   26856 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0229 15:39:32.750607   26856 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0229 15:39:32.750607   26856 cache.go:194] Successfully downloaded all kic artifacts
I0229 15:39:32.750607   26856 start.go:365] acquiring machines lock for minikube-m02: {Name:mk21c175f071556e031c2fc2138cdc3b422dabe0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0229 15:39:32.750607   26856 start.go:369] acquired machines lock for "minikube-m02" in 0s
I0229 15:39:32.750607   26856 start.go:96] Skipping create...Using existing machine configuration
I0229 15:39:32.750607   26856 fix.go:54] fixHost starting: m02
I0229 15:39:32.752768   26856 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0229 15:39:32.859664   26856 fix.go:102] recreateIfNeeded on minikube-m02: state=Stopped err=<nil>
W0229 15:39:32.859664   26856 fix.go:128] unexpected machine state, will restart: <nil>
I0229 15:39:32.860230   26856 out.go:177] 🔄  Restarting existing docker container for "minikube-m02" ...
I0229 15:39:32.861866   26856 cli_runner.go:164] Run: docker start minikube-m02
I0229 15:39:33.191596   26856 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0229 15:39:33.299329   26856 kic.go:430] container "minikube-m02" state is running.
I0229 15:39:33.301442   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0229 15:39:33.423254   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:33.423807   26856 machine.go:88] provisioning docker machine ...
I0229 15:39:33.423807   26856 ubuntu.go:169] provisioning hostname "minikube-m02"
I0229 15:39:33.424854   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:33.531954   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:33.532482   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63718 <nil> <nil>}
I0229 15:39:33.532482   26856 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I0229 15:39:33.698257   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0229 15:39:33.699299   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:33.812559   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:33.812559   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63718 <nil> <nil>}
I0229 15:39:33.812559   26856 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0229 15:39:33.923113   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0229 15:39:33.923113   26856 ubuntu.go:175] set auth options {CertDir:C:\Users\user\.minikube CaCertPath:C:\Users\user\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\user\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\user\.minikube\machines\server.pem ServerKeyPath:C:\Users\user\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\user\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\user\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\user\.minikube}
I0229 15:39:33.923113   26856 ubuntu.go:177] setting up certificates
I0229 15:39:33.923113   26856 provision.go:83] configureAuth start
I0229 15:39:33.924173   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0229 15:39:34.032350   26856 provision.go:138] copyHostCerts
I0229 15:39:34.032350   26856 exec_runner.go:144] found C:\Users\user\.minikube/ca.pem, removing ...
I0229 15:39:34.032350   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\ca.pem
I0229 15:39:34.032350   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\ca.pem --> C:\Users\user\.minikube/ca.pem (1074 bytes)
I0229 15:39:34.032857   26856 exec_runner.go:144] found C:\Users\user\.minikube/cert.pem, removing ...
I0229 15:39:34.032857   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\cert.pem
I0229 15:39:34.032857   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\cert.pem --> C:\Users\user\.minikube/cert.pem (1115 bytes)
I0229 15:39:34.033389   26856 exec_runner.go:144] found C:\Users\user\.minikube/key.pem, removing ...
I0229 15:39:34.033389   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\key.pem
I0229 15:39:34.033389   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\key.pem --> C:\Users\user\.minikube/key.pem (1675 bytes)
I0229 15:39:34.033905   26856 provision.go:112] generating server cert: C:\Users\user\.minikube\machines\server.pem ca-key=C:\Users\user\.minikube\certs\ca.pem private-key=C:\Users\user\.minikube\certs\ca-key.pem org=user.minikube-m02 san=[192.168.49.3 127.0.0.1 localhost 127.0.0.1 minikube minikube-m02]
I0229 15:39:34.207610   26856 provision.go:172] copyRemoteCerts
I0229 15:39:34.215745   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0229 15:39:34.219246   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:34.331016   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63718 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0229 15:39:34.414794   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0229 15:39:34.428760   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server.pem --> /etc/docker/server.pem (1204 bytes)
I0229 15:39:34.442477   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0229 15:39:34.456067   26856 provision.go:86] duration metric: configureAuth took 532.9544ms
I0229 15:39:34.456067   26856 ubuntu.go:193] setting minikube options for container-runtime
I0229 15:39:34.456588   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:34.457106   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:34.562983   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:34.563527   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63718 <nil> <nil>}
I0229 15:39:34.563527   26856 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0229 15:39:34.672326   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0229 15:39:34.672326   26856 ubuntu.go:71] root file system type: overlay
I0229 15:39:34.672326   26856 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0229 15:39:34.673354   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:34.780460   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:34.781044   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63718 <nil> <nil>}
I0229 15:39:34.781044   26856 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0229 15:39:34.898803   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0229 15:39:34.899852   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:34.998300   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:34.998992   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63718 <nil> <nil>}
I0229 15:39:34.998992   26856 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0229 15:39:35.571238   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-02-29 09:17:46.423164491 +0000
+++ /lib/systemd/system/docker.service.new	2024-02-29 12:39:34.881638715 +0000
@@ -12,6 +12,7 @@
 Type=notify
 Restart=on-failure
 
+Environment=NO_PROXY=192.168.49.2
 
 
 # This file is a systemd drop-in unit that inherits from the base dockerd configuration.
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0229 15:39:35.571238   26856 machine.go:91] provisioned docker machine in 2.1474306s
I0229 15:39:35.571238   26856 start.go:300] post-start starting for "minikube-m02" (driver="docker")
I0229 15:39:35.571238   26856 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0229 15:39:35.573354   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0229 15:39:35.574414   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:35.669760   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63718 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0229 15:39:35.766555   26856 ssh_runner.go:195] Run: cat /etc/os-release
I0229 15:39:35.769512   26856 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0229 15:39:35.769512   26856 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0229 15:39:35.769512   26856 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0229 15:39:35.769512   26856 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0229 15:39:35.769512   26856 filesync.go:126] Scanning C:\Users\user\.minikube\addons for local assets ...
I0229 15:39:35.770045   26856 filesync.go:126] Scanning C:\Users\user\.minikube\files for local assets ...
I0229 15:39:35.770045   26856 start.go:303] post-start completed in 198.8073ms
I0229 15:39:35.771641   26856 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0229 15:39:35.772705   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:35.864086   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63718 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0229 15:39:35.944606   26856 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0229 15:39:35.948286   26856 fix.go:56] fixHost completed within 3.1976796s
I0229 15:39:35.948286   26856 start.go:83] releasing machines lock for "minikube-m02", held for 3.1976796s
I0229 15:39:35.951516   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0229 15:39:36.054080   26856 out.go:177] 🌐  Found network options:
I0229 15:39:36.054612   26856 out.go:177]     ▪ NO_PROXY=192.168.49.2
W0229 15:39:36.055137   26856 proxy.go:119] fail to check proxy env: Error ip not in block
I0229 15:39:36.055137   26856 out.go:177]     ▪ NO_PROXY=192.168.49.2
W0229 15:39:36.055660   26856 proxy.go:119] fail to check proxy env: Error ip not in block
W0229 15:39:36.055660   26856 proxy.go:119] fail to check proxy env: Error ip not in block
I0229 15:39:36.056197   26856 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0229 15:39:36.058305   26856 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0229 15:39:36.058305   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:36.058821   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0229 15:39:36.157541   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63718 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0229 15:39:36.169392   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63718 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0229 15:39:36.797466   26856 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0229 15:39:36.803264   26856 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0229 15:39:36.805369   26856 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0229 15:39:36.810763   26856 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0229 15:39:36.811280   26856 start.go:472] detecting cgroup driver to use...
I0229 15:39:36.811280   26856 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0229 15:39:36.811280   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0229 15:39:36.822298   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0229 15:39:36.830349   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0229 15:39:36.836703   26856 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0229 15:39:36.838824   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0229 15:39:36.846722   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0229 15:39:36.854228   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0229 15:39:36.863192   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0229 15:39:36.874452   26856 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0229 15:39:36.882455   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0229 15:39:36.890536   26856 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0229 15:39:36.897279   26856 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0229 15:39:36.904192   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:36.978758   26856 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0229 15:39:37.086316   26856 start.go:472] detecting cgroup driver to use...
I0229 15:39:37.086316   26856 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0229 15:39:37.088418   26856 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0229 15:39:37.095773   26856 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0229 15:39:37.097855   26856 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0229 15:39:37.105757   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0229 15:39:37.117863   26856 ssh_runner.go:195] Run: which cri-dockerd
I0229 15:39:37.123475   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0229 15:39:37.129225   26856 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0229 15:39:37.142236   26856 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0229 15:39:37.230639   26856 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0229 15:39:37.328549   26856 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0229 15:39:37.328549   26856 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0229 15:39:37.342243   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:37.428667   26856 ssh_runner.go:195] Run: sudo systemctl restart docker
I0229 15:39:37.589143   26856 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0229 15:39:37.669298   26856 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0229 15:39:37.708145   26856 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0229 15:39:37.798900   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:37.839093   26856 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0229 15:39:37.852151   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:37.890874   26856 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0229 15:39:37.951134   26856 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0229 15:39:37.953200   26856 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0229 15:39:37.955793   26856 start.go:540] Will wait 60s for crictl version
I0229 15:39:37.957919   26856 ssh_runner.go:195] Run: which crictl
I0229 15:39:37.962653   26856 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0229 15:39:37.993982   26856 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0229 15:39:37.995014   26856 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0229 15:39:38.012308   26856 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0229 15:39:38.028025   26856 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0229 15:39:38.028542   26856 out.go:177]     ▪ env NO_PROXY=192.168.49.2
I0229 15:39:38.030101   26856 cli_runner.go:164] Run: docker exec -t minikube-m02 dig +short host.docker.internal
I0229 15:39:38.172476   26856 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0229 15:39:38.174043   26856 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0229 15:39:38.178915   26856 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0229 15:39:38.185829   26856 certs.go:56] Setting up C:\Users\user\.minikube\profiles\minikube for IP: 192.168.49.3
I0229 15:39:38.185829   26856 certs.go:190] acquiring lock for shared ca certs: {Name:mk7164a99e354388efd17e38c9f3aba55425681b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0229 15:39:38.186346   26856 certs.go:199] skipping minikubeCA CA generation: C:\Users\user\.minikube\ca.key
I0229 15:39:38.186346   26856 certs.go:199] skipping proxyClientCA CA generation: C:\Users\user\.minikube\proxy-client-ca.key
I0229 15:39:38.186346   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\ca-key.pem (1675 bytes)
I0229 15:39:38.186346   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\ca.pem (1074 bytes)
I0229 15:39:38.186346   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\cert.pem (1115 bytes)
I0229 15:39:38.186879   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\key.pem (1675 bytes)
I0229 15:39:38.186879   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0229 15:39:38.200598   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0229 15:39:38.213838   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0229 15:39:38.227191   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0229 15:39:38.240542   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0229 15:39:38.255810   26856 ssh_runner.go:195] Run: openssl version
I0229 15:39:38.261071   26856 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0229 15:39:38.268770   26856 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:38.271346   26856 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb 29 07:47 /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:38.272941   26856 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:38.279294   26856 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0229 15:39:38.286171   26856 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0229 15:39:38.288761   26856 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0229 15:39:38.289790   26856 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0229 15:39:38.325273   26856 cni.go:84] Creating CNI manager for ""
I0229 15:39:38.325273   26856 cni.go:136] 3 nodes found, recommending kindnet
I0229 15:39:38.325273   26856 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0229 15:39:38.325273   26856 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.3 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m02 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.3 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0229 15:39:38.325273   26856 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m02"
  kubeletExtraArgs:
    node-ip: 192.168.49.3
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0229 15:39:38.325273   26856 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.3

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0229 15:39:38.327924   26856 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0229 15:39:38.334229   26856 binaries.go:44] Found k8s binaries, skipping transfer
I0229 15:39:38.335795   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0229 15:39:38.341484   26856 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I0229 15:39:38.351642   26856 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0229 15:39:38.363210   26856 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0229 15:39:38.365787   26856 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0229 15:39:38.371984   26856 host.go:66] Checking if "minikube" exists ...
I0229 15:39:38.372493   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:38.372493   26856 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0229 15:39:38.372493   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I0229 15:39:38.373535   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:38.482284   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:38.597888   26856 start.go:317] removing existing worker node "m02" before attempting to rejoin cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:38.597888   26856 host.go:66] Checking if "minikube" exists ...
I0229 15:39:38.599987   26856 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl drain minikube-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data
I0229 15:39:38.601059   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:38.701467   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:41.853435   26856 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl drain minikube-m02 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data: (3.2534484s)
I0229 15:39:41.853435   26856 node.go:108] successfully drained node "m02"
I0229 15:39:41.861429   26856 node.go:124] successfully deleted node "m02"
I0229 15:39:41.861429   26856 start.go:321] successfully removed existing worker node "m02" from cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:41.861429   26856 start.go:325] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:41.861429   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token gzwbsh.5irxnu6oprug0y8a --discovery-token-ca-cert-hash sha256:8dc435725e7aadb076838f2d380d27ff47efe830e4c65103eb0a94fcf70c3837 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02"
I0229 15:39:48.188772   26856 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token gzwbsh.5irxnu6oprug0y8a --discovery-token-ca-cert-hash sha256:8dc435725e7aadb076838f2d380d27ff47efe830e4c65103eb0a94fcf70c3837 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02": (6.3273431s)
I0229 15:39:48.188772   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0229 15:39:48.344915   26856 start.go:306] JoinCluster complete in 9.9724217s
I0229 15:39:48.344915   26856 cni.go:84] Creating CNI manager for ""
I0229 15:39:48.344915   26856 cni.go:136] 3 nodes found, recommending kindnet
I0229 15:39:48.346995   26856 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0229 15:39:48.350670   26856 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0229 15:39:48.350670   26856 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0229 15:39:48.362877   26856 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0229 15:39:48.506368   26856 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0229 15:39:48.506368   26856 start.go:223] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:48.506871   26856 out.go:177] 🔎  Verifying Kubernetes components...
I0229 15:39:48.510639   26856 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0229 15:39:48.519629   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0229 15:39:48.614836   26856 kubeadm.go:581] duration metric: took 108.4672ms to wait for : map[apiserver:true system_pods:true] ...
I0229 15:39:48.614836   26856 node_conditions.go:102] verifying NodePressure condition ...
I0229 15:39:48.618656   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:48.618656   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:48.618656   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:48.618656   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:48.618656   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:48.618656   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:48.618656   26856 node_conditions.go:105] duration metric: took 3.8203ms to run NodePressure ...
I0229 15:39:48.618656   26856 start.go:228] waiting for startup goroutines ...
I0229 15:39:48.618656   26856 start.go:242] writing updated cluster config ...
I0229 15:39:48.624478   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:48.625005   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:48.626062   26856 out.go:177] 👍  Starting worker node minikube-m03 in cluster minikube
I0229 15:39:48.626617   26856 cache.go:121] Beginning downloading kic base image for docker with docker
I0229 15:39:48.626617   26856 out.go:177] 🚜  Pulling base image ...
I0229 15:39:48.627142   26856 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0229 15:39:48.627142   26856 cache.go:56] Caching tarball of preloaded images
I0229 15:39:48.627142   26856 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0229 15:39:48.627671   26856 preload.go:174] Found C:\Users\user\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0229 15:39:48.627671   26856 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0229 15:39:48.627671   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:48.738991   26856 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0229 15:39:48.738991   26856 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0229 15:39:48.738991   26856 cache.go:194] Successfully downloaded all kic artifacts
I0229 15:39:48.738991   26856 start.go:365] acquiring machines lock for minikube-m03: {Name:mk2acaee88750e363920fbc7125255c47595a5d3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0229 15:39:48.738991   26856 start.go:369] acquired machines lock for "minikube-m03" in 0s
I0229 15:39:48.738991   26856 start.go:96] Skipping create...Using existing machine configuration
I0229 15:39:48.738991   26856 fix.go:54] fixHost starting: m03
I0229 15:39:48.741111   26856 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0229 15:39:48.848025   26856 fix.go:102] recreateIfNeeded on minikube-m03: state=Stopped err=<nil>
W0229 15:39:48.848025   26856 fix.go:128] unexpected machine state, will restart: <nil>
I0229 15:39:48.848025   26856 out.go:177] 🔄  Restarting existing docker container for "minikube-m03" ...
I0229 15:39:48.850203   26856 cli_runner.go:164] Run: docker start minikube-m03
I0229 15:39:49.192054   26856 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0229 15:39:49.300803   26856 kic.go:430] container "minikube-m03" state is running.
I0229 15:39:49.302964   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0229 15:39:49.424571   26856 profile.go:148] Saving config to C:\Users\user\.minikube\profiles\minikube\config.json ...
I0229 15:39:49.425652   26856 machine.go:88] provisioning docker machine ...
I0229 15:39:49.425652   26856 ubuntu.go:169] provisioning hostname "minikube-m03"
I0229 15:39:49.427244   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:49.533957   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:49.533957   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63742 <nil> <nil>}
I0229 15:39:49.533957   26856 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I0229 15:39:49.658430   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0229 15:39:49.659484   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:49.768235   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:49.768304   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63742 <nil> <nil>}
I0229 15:39:49.768304   26856 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I0229 15:39:49.871890   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0229 15:39:49.871890   26856 ubuntu.go:175] set auth options {CertDir:C:\Users\user\.minikube CaCertPath:C:\Users\user\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\user\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\user\.minikube\machines\server.pem ServerKeyPath:C:\Users\user\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\user\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\user\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\user\.minikube}
I0229 15:39:49.871890   26856 ubuntu.go:177] setting up certificates
I0229 15:39:49.871890   26856 provision.go:83] configureAuth start
I0229 15:39:49.872934   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0229 15:39:49.970000   26856 provision.go:138] copyHostCerts
I0229 15:39:49.970000   26856 exec_runner.go:144] found C:\Users\user\.minikube/ca.pem, removing ...
I0229 15:39:49.970000   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\ca.pem
I0229 15:39:49.970000   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\ca.pem --> C:\Users\user\.minikube/ca.pem (1074 bytes)
I0229 15:39:49.970548   26856 exec_runner.go:144] found C:\Users\user\.minikube/cert.pem, removing ...
I0229 15:39:49.970548   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\cert.pem
I0229 15:39:49.970548   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\cert.pem --> C:\Users\user\.minikube/cert.pem (1115 bytes)
I0229 15:39:49.971093   26856 exec_runner.go:144] found C:\Users\user\.minikube/key.pem, removing ...
I0229 15:39:49.971093   26856 exec_runner.go:203] rm: C:\Users\user\.minikube\key.pem
I0229 15:39:49.971093   26856 exec_runner.go:151] cp: C:\Users\user\.minikube\certs\key.pem --> C:\Users\user\.minikube/key.pem (1675 bytes)
I0229 15:39:49.971093   26856 provision.go:112] generating server cert: C:\Users\user\.minikube\machines\server.pem ca-key=C:\Users\user\.minikube\certs\ca.pem private-key=C:\Users\user\.minikube\certs\ca-key.pem org=user.minikube-m03 san=[192.168.49.4 127.0.0.1 localhost 127.0.0.1 minikube minikube-m03]
I0229 15:39:50.053560   26856 provision.go:172] copyRemoteCerts
I0229 15:39:50.053560   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0229 15:39:50.063246   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:50.174413   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63742 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0229 15:39:50.254707   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0229 15:39:50.268148   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0229 15:39:50.281496   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\machines\server.pem --> /etc/docker/server.pem (1204 bytes)
I0229 15:39:50.294394   26856 provision.go:86] duration metric: configureAuth took 422.5042ms
I0229 15:39:50.294394   26856 ubuntu.go:193] setting minikube options for container-runtime
I0229 15:39:50.294394   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:50.295424   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:50.392687   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:50.393215   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63742 <nil> <nil>}
I0229 15:39:50.393215   26856 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0229 15:39:50.501822   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0229 15:39:50.501822   26856 ubuntu.go:71] root file system type: overlay
I0229 15:39:50.501822   26856 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0229 15:39:50.502880   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:50.610401   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:50.610983   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63742 <nil> <nil>}
I0229 15:39:50.610983   26856 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0229 15:39:50.726835   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0229 15:39:50.727875   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:50.828424   26856 main.go:141] libmachine: Using SSH client type: native
I0229 15:39:50.828961   26856 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xb947e0] 0xb97320 <nil>  [] 0s} 127.0.0.1 63742 <nil> <nil>}
I0229 15:39:50.828961   26856 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0229 15:39:51.345904   26856 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-02-29 09:18:11.893748841 +0000
+++ /lib/systemd/system/docker.service.new	2024-02-29 12:39:50.700001364 +0000
@@ -12,6 +12,8 @@
 Type=notify
 Restart=on-failure
 
+Environment=NO_PROXY=192.168.49.2
+Environment=NO_PROXY=192.168.49.2,192.168.49.3
 
 
 # This file is a systemd drop-in unit that inherits from the base dockerd configuration.
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0229 15:39:51.345904   26856 machine.go:91] provisioned docker machine in 1.9202518s
I0229 15:39:51.345904   26856 start.go:300] post-start starting for "minikube-m03" (driver="docker")
I0229 15:39:51.345904   26856 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0229 15:39:51.347974   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0229 15:39:51.349014   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:51.452086   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63742 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0229 15:39:51.543286   26856 ssh_runner.go:195] Run: cat /etc/os-release
I0229 15:39:51.545931   26856 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0229 15:39:51.545931   26856 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0229 15:39:51.545931   26856 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0229 15:39:51.545931   26856 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0229 15:39:51.545931   26856 filesync.go:126] Scanning C:\Users\user\.minikube\addons for local assets ...
I0229 15:39:51.545931   26856 filesync.go:126] Scanning C:\Users\user\.minikube\files for local assets ...
I0229 15:39:51.545931   26856 start.go:303] post-start completed in 200.0274ms
I0229 15:39:51.547550   26856 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0229 15:39:51.548641   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:51.653005   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63742 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0229 15:39:51.750817   26856 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0229 15:39:51.753950   26856 fix.go:56] fixHost completed within 3.0149589s
I0229 15:39:51.753950   26856 start.go:83] releasing machines lock for "minikube-m03", held for 3.0149589s
I0229 15:39:51.754981   26856 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0229 15:39:51.853893   26856 out.go:177] 🌐  Found network options:
I0229 15:39:51.854423   26856 out.go:177]     ▪ NO_PROXY=192.168.49.2,192.168.49.3
W0229 15:39:51.854964   26856 proxy.go:119] fail to check proxy env: Error ip not in block
W0229 15:39:51.854964   26856 proxy.go:119] fail to check proxy env: Error ip not in block
I0229 15:39:51.854964   26856 out.go:177]     ▪ NO_PROXY=192.168.49.2,192.168.49.3
W0229 15:39:51.855483   26856 proxy.go:119] fail to check proxy env: Error ip not in block
W0229 15:39:51.855483   26856 proxy.go:119] fail to check proxy env: Error ip not in block
W0229 15:39:51.855483   26856 proxy.go:119] fail to check proxy env: Error ip not in block
W0229 15:39:51.855483   26856 proxy.go:119] fail to check proxy env: Error ip not in block
I0229 15:39:51.856001   26856 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0229 15:39:51.857053   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:51.859175   26856 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0229 15:39:51.860762   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0229 15:39:51.960745   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63742 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0229 15:39:51.971972   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63742 SSHKeyPath:C:\Users\user\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0229 15:39:52.676961   26856 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0229 15:39:52.683811   26856 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0229 15:39:52.686487   26856 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0229 15:39:52.691934   26856 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0229 15:39:52.691934   26856 start.go:472] detecting cgroup driver to use...
I0229 15:39:52.691934   26856 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0229 15:39:52.691934   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0229 15:39:52.704062   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0229 15:39:52.711583   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0229 15:39:52.717411   26856 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0229 15:39:52.719550   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0229 15:39:52.727284   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0229 15:39:52.735753   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0229 15:39:52.743267   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0229 15:39:52.750989   26856 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0229 15:39:52.758459   26856 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0229 15:39:52.765903   26856 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0229 15:39:52.773273   26856 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0229 15:39:52.780662   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:52.859957   26856 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0229 15:39:52.936838   26856 start.go:472] detecting cgroup driver to use...
I0229 15:39:52.936838   26856 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0229 15:39:52.938894   26856 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0229 15:39:52.946756   26856 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0229 15:39:52.948326   26856 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0229 15:39:52.956742   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0229 15:39:52.968941   26856 ssh_runner.go:195] Run: which cri-dockerd
I0229 15:39:52.974754   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0229 15:39:52.980722   26856 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0229 15:39:52.993233   26856 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0229 15:39:53.070408   26856 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0229 15:39:53.149776   26856 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0229 15:39:53.149776   26856 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0229 15:39:53.162788   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:53.241577   26856 ssh_runner.go:195] Run: sudo systemctl restart docker
I0229 15:39:53.396819   26856 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0229 15:39:53.469771   26856 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0229 15:39:53.509207   26856 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0229 15:39:53.580381   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:53.659774   26856 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0229 15:39:53.672142   26856 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0229 15:39:53.759465   26856 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0229 15:39:53.810260   26856 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0229 15:39:53.812477   26856 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0229 15:39:53.816140   26856 start.go:540] Will wait 60s for crictl version
I0229 15:39:53.817681   26856 ssh_runner.go:195] Run: which crictl
I0229 15:39:53.822416   26856 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0229 15:39:53.854245   26856 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0229 15:39:53.855285   26856 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0229 15:39:53.873227   26856 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0229 15:39:53.890292   26856 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0229 15:39:53.890810   26856 out.go:177]     ▪ env NO_PROXY=192.168.49.2
I0229 15:39:53.891339   26856 out.go:177]     ▪ env NO_PROXY=192.168.49.2,192.168.49.3
I0229 15:39:53.892421   26856 cli_runner.go:164] Run: docker exec -t minikube-m03 dig +short host.docker.internal
I0229 15:39:54.035359   26856 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0229 15:39:54.038095   26856 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0229 15:39:54.042438   26856 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0229 15:39:54.049371   26856 certs.go:56] Setting up C:\Users\user\.minikube\profiles\minikube for IP: 192.168.49.4
I0229 15:39:54.049371   26856 certs.go:190] acquiring lock for shared ca certs: {Name:mk7164a99e354388efd17e38c9f3aba55425681b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0229 15:39:54.049371   26856 certs.go:199] skipping minikubeCA CA generation: C:\Users\user\.minikube\ca.key
I0229 15:39:54.049891   26856 certs.go:199] skipping proxyClientCA CA generation: C:\Users\user\.minikube\proxy-client-ca.key
I0229 15:39:54.049891   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\ca-key.pem (1675 bytes)
I0229 15:39:54.049891   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\ca.pem (1074 bytes)
I0229 15:39:54.050406   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\cert.pem (1115 bytes)
I0229 15:39:54.050463   26856 certs.go:437] found cert: C:\Users\user\.minikube\certs\C:\Users\user\.minikube\certs\key.pem (1675 bytes)
I0229 15:39:54.050987   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0229 15:39:54.064504   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0229 15:39:54.078191   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0229 15:39:54.091672   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0229 15:39:54.105057   26856 ssh_runner.go:362] scp C:\Users\user\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0229 15:39:54.121123   26856 ssh_runner.go:195] Run: openssl version
I0229 15:39:54.127948   26856 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0229 15:39:54.136272   26856 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:54.138868   26856 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb 29 07:47 /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:54.140965   26856 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0229 15:39:54.148508   26856 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0229 15:39:54.155850   26856 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0229 15:39:54.158964   26856 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0229 15:39:54.160018   26856 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0229 15:39:54.195918   26856 cni.go:84] Creating CNI manager for ""
I0229 15:39:54.195918   26856 cni.go:136] 3 nodes found, recommending kindnet
I0229 15:39:54.195918   26856 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0229 15:39:54.195918   26856 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.4 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m03 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.4 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0229 15:39:54.195918   26856 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.4
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m03"
  kubeletExtraArgs:
    node-ip: 192.168.49.4
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0229 15:39:54.195918   26856 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m03 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.4

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0229 15:39:54.198034   26856 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0229 15:39:54.204269   26856 binaries.go:44] Found k8s binaries, skipping transfer
I0229 15:39:54.205850   26856 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0229 15:39:54.211553   26856 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I0229 15:39:54.221716   26856 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0229 15:39:54.233925   26856 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0229 15:39:54.236765   26856 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0229 15:39:54.243516   26856 host.go:66] Checking if "minikube" exists ...
I0229 15:39:54.244043   26856 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0229 15:39:54.244043   26856 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\user:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0229 15:39:54.244043   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I0229 15:39:54.245082   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:54.345601   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:54.460065   26856 start.go:317] removing existing worker node "m03" before attempting to rejoin cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:54.460065   26856 host.go:66] Checking if "minikube" exists ...
I0229 15:39:54.462277   26856 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl drain minikube-m03 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data
I0229 15:39:54.463320   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0229 15:39:54.562405   26856 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63673 SSHKeyPath:C:\Users\user\.minikube\machines\minikube\id_rsa Username:docker}
I0229 15:39:57.732027   26856 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl drain minikube-m03 --force --grace-period=1 --skip-wait-for-delete-timeout=1 --disable-eviction --ignore-daemonsets --delete-emptydir-data --delete-local-data: (3.2697501s)
I0229 15:39:57.732027   26856 node.go:108] successfully drained node "m03"
I0229 15:39:57.736715   26856 node.go:124] successfully deleted node "m03"
I0229 15:39:57.736715   26856 start.go:321] successfully removed existing worker node "m03" from cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:57.736715   26856 start.go:325] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:57.736715   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 0e4re1.wt3chitf7phjk887 --discovery-token-ca-cert-hash sha256:8dc435725e7aadb076838f2d380d27ff47efe830e4c65103eb0a94fcf70c3837 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03"
I0229 15:39:58.745184   26856 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 0e4re1.wt3chitf7phjk887 --discovery-token-ca-cert-hash sha256:8dc435725e7aadb076838f2d380d27ff47efe830e4c65103eb0a94fcf70c3837 --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03": (1.0079526s)
I0229 15:39:58.745184   26856 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0229 15:39:58.899179   26856 start.go:306] JoinCluster complete in 4.6551358s
I0229 15:39:58.899179   26856 cni.go:84] Creating CNI manager for ""
I0229 15:39:58.899179   26856 cni.go:136] 3 nodes found, recommending kindnet
I0229 15:39:58.900746   26856 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0229 15:39:58.904118   26856 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0229 15:39:58.904118   26856 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0229 15:39:58.916747   26856 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0229 15:39:59.058665   26856 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0229 15:39:59.058665   26856 start.go:223] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime: ControlPlane:false Worker:true}
I0229 15:39:59.059767   26856 out.go:177] 🔎  Verifying Kubernetes components...
I0229 15:39:59.062891   26856 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0229 15:39:59.072621   26856 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0229 15:39:59.171226   26856 kubeadm.go:581] duration metric: took 112.5602ms to wait for : map[apiserver:true system_pods:true] ...
I0229 15:39:59.171226   26856 node_conditions.go:102] verifying NodePressure condition ...
I0229 15:39:59.174586   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:59.174586   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:59.174586   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:59.174586   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:59.174586   26856 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0229 15:39:59.174586   26856 node_conditions.go:123] node cpu capacity is 16
I0229 15:39:59.174586   26856 node_conditions.go:105] duration metric: took 3.3608ms to run NodePressure ...
I0229 15:39:59.174586   26856 start.go:228] waiting for startup goroutines ...
I0229 15:39:59.174586   26856 start.go:242] writing updated cluster config ...
I0229 15:39:59.176698   26856 ssh_runner.go:195] Run: rm -f paused
I0229 15:39:59.249162   26856 start.go:600] kubectl: 1.29.1, cluster: 1.28.3 (minor skew: 1)
I0229 15:39:59.249742   26856 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 29 12:39:14 minikube dockerd[778]: time="2024-02-29T12:39:14.676100962Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Feb 29 12:39:14 minikube systemd[1]: docker.service: Deactivated successfully.
Feb 29 12:39:14 minikube systemd[1]: Stopped Docker Application Container Engine.
Feb 29 12:39:14 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.705259476Z" level=info msg="Starting up"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.713022693Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.721654039Z" level=info msg="Loading containers: start."
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.801849475Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.822847662Z" level=info msg="Loading containers: done."
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.831708952Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.831737472Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.831742414Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.831745261Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.831757022Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.831790636Z" level=info msg="Daemon has completed initialization"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.843524868Z" level=info msg="API listen on /var/run/docker.sock"
Feb 29 12:39:14 minikube dockerd[1000]: time="2024-02-29T12:39:14.843606089Z" level=info msg="API listen on [::]:2376"
Feb 29 12:39:14 minikube systemd[1]: Started Docker Application Container Engine.
Feb 29 12:39:15 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Start docker client with request timeout 0s"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Loaded network plugin cni"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Docker Info: &{ID:2ddf3725-ead8-4d76-a858-2a80c05e3a30 Containers:33 ContainersRunning:0 ContainersPaused:0 ContainersStopped:33 Images:10 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:35 SystemTime:2024-02-29T12:39:15.255829619Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.133.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00002b2d0 NCPU:16 MemTotal:16626552832 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 29 12:39:15 minikube cri-dockerd[1236]: time="2024-02-29T12:39:15Z" level=info msg="Start cri-dockerd grpc backend"
Feb 29 12:39:15 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-6q95s_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cbf6206c2d850c17a2e042a4d07fb6ab4a8a6fd9de324c1adcdf44e082edec83\""
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-6q95s_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"d00bbf5db98226ba83d383f20906f0672996287a9944120748133bb51319924f\""
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6c57a8e73512c891329ad3caa31448d4c4147a619e132744eacffafdfd32b8b7\""
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"546160c5fa55fc59e43f821395482aa9c1806a6e4171a41a9f4ebe097efa1635\""
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"e7061be6543ed9a47d38384d6559894632e1c00bfb01218a0f3ed215b08c2dae\". Proceed without further sandbox information."
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"640c2cee93bc8a79eedf1bdc1aa8a4754861c1f65db447acfe85705920c18ca6\". Proceed without further sandbox information."
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"0816f03c7a6c7b1db5daad34c9c661a3dae8251cdfe5d4e50b064099fd39fc6b\". Proceed without further sandbox information."
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"6bd169280e2c0b652717633d9e67f368e81dcb04b2e9838d070dd3f834790f9a\". Proceed without further sandbox information."
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7acf073d9e786c7498233a25dd35b1b88872995cbecf472d8909c4799c6468d1/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c63f4f211d71452ad9be0282128adb28c94a1e04fbc5652f234bb3cd4032a312/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/008c87a959c50b063c4fc25972e54cad459b5989886b577e9c0aeba3ac9aef1f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:27 minikube cri-dockerd[1236]: time="2024-02-29T12:39:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/df4d2f792c2ac0d5d20b5ae30d44586ee955c241208d5aefbce03d7798dc55dc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:28 minikube cri-dockerd[1236]: time="2024-02-29T12:39:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-6q95s_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cbf6206c2d850c17a2e042a4d07fb6ab4a8a6fd9de324c1adcdf44e082edec83\""
Feb 29 12:39:28 minikube cri-dockerd[1236]: time="2024-02-29T12:39:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6c57a8e73512c891329ad3caa31448d4c4147a619e132744eacffafdfd32b8b7\""
Feb 29 12:39:29 minikube cri-dockerd[1236]: time="2024-02-29T12:39:29Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 29 12:39:30 minikube cri-dockerd[1236]: time="2024-02-29T12:39:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/099c5805b6d7076b543871654c5e02dce922f05a5791fda4636c073624b216c3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:30 minikube cri-dockerd[1236]: time="2024-02-29T12:39:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/94da55d79656683e41bebc42f0eae5d06db8ccac14c13e7b13c1fb78c88484df/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:30 minikube cri-dockerd[1236]: time="2024-02-29T12:39:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e0c7c94afafb6d29cdc87e2247bc66363b8c76abce0292ecca4f172d29576f65/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:30 minikube cri-dockerd[1236]: time="2024-02-29T12:39:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/57988ced634a4c96c7111863e8f7c0da04946698190863b2a5ed204d60fef7c7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 29 12:39:30 minikube cri-dockerd[1236]: time="2024-02-29T12:39:30Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/09709fadc9154a0468edab4fdfac22f5d5d73455dd6c05c4dae8408f313a5f79/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 29 12:39:40 minikube dockerd[1000]: time="2024-02-29T12:39:40.640832825Z" level=info msg="ignoring event" container=350d35d053fe4c8da5b09ccb03c6434400f1965f23804614e672b395d4add327 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 29 12:39:42 minikube dockerd[1000]: time="2024-02-29T12:39:42.033358060Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout"
Feb 29 12:39:42 minikube dockerd[1000]: time="2024-02-29T12:39:42.035241872Z" level=error msg="Handler for POST /v1.42/images/create returned error: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout"
Feb 29 12:40:04 minikube dockerd[1000]: time="2024-02-29T12:40:04.752291555Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout"
Feb 29 12:40:04 minikube dockerd[1000]: time="2024-02-29T12:40:04.758571016Z" level=error msg="Handler for POST /v1.42/images/create returned error: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout"
Feb 29 12:40:40 minikube dockerd[1000]: time="2024-02-29T12:40:40.803265270Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout"
Feb 29 12:40:40 minikube dockerd[1000]: time="2024-02-29T12:40:40.805795371Z" level=error msg="Handler for POST /v1.42/images/create returned error: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout"
Feb 29 12:41:43 minikube dockerd[1000]: time="2024-02-29T12:41:43.116896924Z" level=warning msg="Error getting v2 registry: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout"
Feb 29 12:41:43 minikube dockerd[1000]: time="2024-02-29T12:41:43.116935003Z" level=info msg="Attempting next endpoint for pull after error: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout"
Feb 29 12:41:43 minikube dockerd[1000]: time="2024-02-29T12:41:43.122854422Z" level=error msg="Handler for POST /v1.42/images/create returned error: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout"


==> container status <==
CONTAINER           IMAGE                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
65506d7bc2d1a       6e38f40d628db                                                                              2 minutes ago       Running             storage-provisioner       12                  94da55d796566       storage-provisioner
7b077367a4494       c7d1297425461                                                                              3 minutes ago       Running             kindnet-cni               1                   09709fadc9154       kindnet-2tprc
ea9b0fa2619c9       ead0a4a53df89                                                                              3 minutes ago       Running             coredns                   6                   e0c7c94afafb6       coredns-5dd5756b68-6q95s
350d35d053fe4       6e38f40d628db                                                                              3 minutes ago       Exited              storage-provisioner       11                  94da55d796566       storage-provisioner
86f92ab350d75       bfc896cf80fba                                                                              3 minutes ago       Running             kube-proxy                6                   099c5805b6d70       kube-proxy-qrhwp
0cac218d49613       6d1b4fd1b182d                                                                              3 minutes ago       Running             kube-scheduler            6                   df4d2f792c2ac       kube-scheduler-minikube
803b1c5193571       10baa1ca17068                                                                              3 minutes ago       Running             kube-controller-manager   6                   008c87a959c50       kube-controller-manager-minikube
7711fb501610b       5374347291230                                                                              3 minutes ago       Running             kube-apiserver            6                   c63f4f211d714       kube-apiserver-minikube
990412c9161b9       73deb9a3f7025                                                                              3 minutes ago       Running             etcd                      6                   7acf073d9e786       etcd-minikube
06c7c2f6eadcf       kindest/kindnetd@sha256:4a58d1cd2b45bf2460762a51a4aa9c80861f460af35800c05baab0573f923052   3 hours ago         Exited              kindnet-cni               0                   8cf8442b0d860       kindnet-2tprc
fcd97146e9b57       nginx@sha256:c26ae7472d624ba1fafd296e73cecc4f93f853088e6a9c13c0d52f6ca5865107              3 hours ago         Exited              nginx                     3                   6c57a8e73512c       nginx
db93da1f747f6       ead0a4a53df89                                                                              3 hours ago         Exited              coredns                   5                   cbf6206c2d850       coredns-5dd5756b68-6q95s
7ac9084a2ca98       5374347291230                                                                              3 hours ago         Exited              kube-apiserver            5                   15572e6e4bf69       kube-apiserver-minikube
ffeec317e3471       10baa1ca17068                                                                              3 hours ago         Exited              kube-controller-manager   5                   c8edfd8945c45       kube-controller-manager-minikube
9b30abcf576c2       6d1b4fd1b182d                                                                              3 hours ago         Exited              kube-scheduler            5                   14b8ccee12575       kube-scheduler-minikube
5f6d7711783ad       73deb9a3f7025                                                                              3 hours ago         Exited              etcd                      5                   d7a5ac5208233       etcd-minikube
695dde55572e3       bfc896cf80fba                                                                              3 hours ago         Exited              kube-proxy                5                   fee1ad122a5ab       kube-proxy-qrhwp


==> coredns [db93da1f747f] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:44819 - 62269 "HINFO IN 5760705864988456864.8794725331794453470. udp 57 false 512" NXDOMAIN qr,rd,ra 57 1.164710399s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [ea9b0fa2619c] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:40248 - 6025 "HINFO IN 5951953306487829572.2170945610496601837. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.558271486s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_02_29T10_47_36_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 29 Feb 2024 07:47:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 29 Feb 2024 12:42:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 29 Feb 2024 12:39:29 +0000   Thu, 29 Feb 2024 07:47:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 29 Feb 2024 12:39:29 +0000   Thu, 29 Feb 2024 07:47:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 29 Feb 2024 12:39:29 +0000   Thu, 29 Feb 2024 07:47:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 29 Feb 2024 12:39:29 +0000   Thu, 29 Feb 2024 07:47:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16236868Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16236868Ki
  pods:               110
System Info:
  Machine ID:                 135b63a47f10431b9761ad899baf5ca8
  System UUID:                135b63a47f10431b9761ad899baf5ca8
  Boot ID:                    ccb242f5-bc77-4b68-b602-9c47e601b717
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     nginx                               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h45m
  kube-system                 coredns-5dd5756b68-6q95s            100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     4h54m
  kube-system                 etcd-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         4h54m
  kube-system                 kindnet-2tprc                       100m (0%!)(MISSING)     100m (0%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      3h24m
  kube-system                 kube-apiserver-minikube             250m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h54m
  kube-system                 kube-controller-manager-minikube    200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h54m
  kube-system                 kube-proxy-qrhwp                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h54m
  kube-system                 kube-scheduler-minikube             100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h54m
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4h54m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (5%!)(MISSING)   100m (0%!)(MISSING)
  memory             220Mi (1%!)(MISSING)  220Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                  From             Message
  ----    ------                   ----                 ----             -------
  Normal  Starting                 3m4s                 kube-proxy       
  Normal  Starting                 3m8s                 kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  3m8s (x8 over 3m8s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    3m8s (x8 over 3m8s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     3m8s (x7 over 3m8s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  3m8s                 kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           2m53s                node-controller  Node minikube event: Registered Node minikube in Controller


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 29 Feb 2024 12:39:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Thu, 29 Feb 2024 12:42:32 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 29 Feb 2024 12:39:48 +0000   Thu, 29 Feb 2024 12:39:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 29 Feb 2024 12:39:48 +0000   Thu, 29 Feb 2024 12:39:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 29 Feb 2024 12:39:48 +0000   Thu, 29 Feb 2024 12:39:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 29 Feb 2024 12:39:48 +0000   Thu, 29 Feb 2024 12:39:48 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16236868Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16236868Ki
  pods:               110
System Info:
  Machine ID:                 05c8256351d843cb95dff5f466564939
  System UUID:                05c8256351d843cb95dff5f466564939
  Boot ID:                    ccb242f5-bc77-4b68-b602-9c47e601b717
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (5 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     postgres-dc95b6f47-g8p49                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m41s
  kube-system                 kindnet-7jnsb                                 100m (0%!)(MISSING)     100m (0%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      3h24m
  kube-system                 kube-proxy-ssxxr                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h24m
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-jr8ln    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m41s
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-z9bk8         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2m41s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%!)(MISSING)  100m (0%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From             Message
  ----    ------                   ----                   ----             -------
  Normal  Starting                 2m44s                  kube-proxy       
  Normal  NodeAllocatableEnforced  3m2s                   kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 3m2s                   kubelet          Starting kubelet.
  Normal  NodeNotSchedulable       2m55s                  kubelet          Node minikube-m02 status is now: NodeNotSchedulable
  Normal  NodeHasSufficientPID     2m55s (x7 over 3m2s)   kubelet          Node minikube-m02 status is now: NodeHasSufficientPID
  Normal  NodeHasNoDiskPressure    2m55s (x7 over 3m2s)   kubelet          Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientMemory  2m53s (x8 over 3m2s)   kubelet          Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal  Starting                 2m48s                  kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  2m48s (x2 over 2m48s)  kubelet          Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m48s (x2 over 2m48s)  kubelet          Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m48s (x2 over 2m48s)  kubelet          Node minikube-m02 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  2m48s                  kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                2m47s                  kubelet          Node minikube-m02 status is now: NodeReady
  Normal  RegisteredNode           2m44s                  node-controller  Node minikube-m02 event: Registered Node minikube-m02 in Controller


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 29 Feb 2024 12:39:58 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Thu, 29 Feb 2024 12:42:31 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 29 Feb 2024 12:39:58 +0000   Thu, 29 Feb 2024 12:39:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 29 Feb 2024 12:39:58 +0000   Thu, 29 Feb 2024 12:39:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 29 Feb 2024 12:39:58 +0000   Thu, 29 Feb 2024 12:39:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 29 Feb 2024 12:39:58 +0000   Thu, 29 Feb 2024 12:39:58 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.4
  Hostname:    minikube-m03
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16236868Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16236868Ki
  pods:               110
System Info:
  Machine ID:                 d0dbdbf1540444b78e964283a0467cb9
  System UUID:                d0dbdbf1540444b78e964283a0467cb9
  Boot ID:                    ccb242f5-bc77-4b68-b602-9c47e601b717
  Kernel Version:             5.15.133.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                ------------  ----------  ---------------  -------------  ---
  kube-system                 kindnet-9ndrz       100m (0%!)(MISSING)     100m (0%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      3h24m
  kube-system                 kube-proxy-lmbbh    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3h24m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%!)(MISSING)  100m (0%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                    From        Message
  ----    ------                   ----                   ----        -------
  Normal  Starting                 2m35s                  kube-proxy  
  Normal  Starting                 2m37s                  kubelet     Starting kubelet.
  Normal  NodeHasSufficientMemory  2m37s (x2 over 2m37s)  kubelet     Node minikube-m03 status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    2m37s (x2 over 2m37s)  kubelet     Node minikube-m03 status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     2m37s (x2 over 2m37s)  kubelet     Node minikube-m03 status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  2m37s                  kubelet     Updated Node Allocatable limit across pods
  Normal  NodeReady                2m37s                  kubelet     Node minikube-m03 status is now: NodeReady


==> dmesg <==
[  +0.050506] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000449] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000384] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000469] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000846] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000433] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000389] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000566] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.063229] /sbin/ldconfig: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.017128] WSL (1) ERROR: ConfigApplyWindowsLibPath:2527: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.005797] WSL (1) WARNING: /usr/share/zoneinfo/Africa/Addis_Ababa not found. Is the tzdata package installed?
[  +0.054632] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000529] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000450] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000452] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000779] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000363] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000365] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000434] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.148221] netlink: 'init': attribute type 4 has an invalid length.
[  +0.155301] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[Feb29 10:21] CPU: 13 PID: 306 Comm: Xwayland Not tainted 5.15.133.1-microsoft-standard-WSL2 #1
[  +0.000003] RIP: 0033:0x7f21422b8e6c
[  +0.000004] Code: ff ff 0f 46 ea eb 99 0f 1f 80 00 00 00 00 b8 ba 00 00 00 0f 05 89 c5 e8 32 d5 04 00 44 89 e2 89 ee 89 c7 b8 ea 00 00 00 0f 05 <89> c5 f7 dd 3d 00 f0 ff ff b8 00 00 00 00 0f 47 c5 48 83 ec 80 5b
[  +0.000002] RSP: 002b:00007ffc13b5f260 EFLAGS: 00000246 ORIG_RAX: 00000000000000ea
[  +0.000002] RAX: 0000000000000000 RBX: 00007f2141c09980 RCX: 00007f21422b8e6c
[  +0.000001] RDX: 0000000000000006 RSI: 0000000000000022 RDI: 0000000000000022
[  +0.000001] RBP: 0000000000000022 R08: 00007ffc13b5f328 R09: 0000000000000000
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 0000000000000006
[  +0.000001] R13: 00007ffc13b5f880 R14: 00007ffc13b5f968 R15: 0000000000000001
[  +0.000001] FS:  00007f2141c09980 GS:  0000000000000000
[  +0.001843] CPU: 8 PID: 348 Comm: Xwayland Not tainted 5.15.133.1-microsoft-standard-WSL2 #1
[  +0.000002] RIP: 0033:0x7f45264a7e6c
[  +0.000003] Code: ff ff 0f 46 ea eb 99 0f 1f 80 00 00 00 00 b8 ba 00 00 00 0f 05 89 c5 e8 32 d5 04 00 44 89 e2 89 ee 89 c7 b8 ea 00 00 00 0f 05 <89> c5 f7 dd 3d 00 f0 ff ff b8 00 00 00 00 0f 47 c5 48 83 ec 80 5b
[  +0.000001] RSP: 002b:00007ffe3be112f0 EFLAGS: 00000246 ORIG_RAX: 00000000000000ea
[  +0.000003] RAX: 0000000000000000 RBX: 00007f4525df8980 RCX: 00007f45264a7e6c
[  +0.000000] RDX: 0000000000000006 RSI: 0000000000000028 RDI: 0000000000000028
[  +0.000001] RBP: 0000000000000028 R08: 00007ffe3be113b8 R09: 0000000000000000
[  +0.000001] R10: 0000000000000008 R11: 0000000000000246 R12: 0000000000000006
[  +0.000001] R13: 00007ffe3be11910 R14: 00007ffe3be119f8 R15: 0000000000000001
[  +0.000000] FS:  00007f4525df8980 GS:  0000000000000000
[  +0.046006] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000640] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000269] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000167] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000816] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000378] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000461] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000781] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000033] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000679] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000251] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000089] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000281] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000327] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000180] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000567] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2


==> etcd [5f6d7711783a] <==
{"level":"info","ts":"2024-02-29T11:18:23.385966Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11323,"took":"424.504µs","hash":398835106}
{"level":"info","ts":"2024-02-29T11:18:23.385997Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":398835106,"revision":11323,"compact-revision":10980}
{"level":"info","ts":"2024-02-29T11:23:23.394762Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11624}
{"level":"info","ts":"2024-02-29T11:23:23.395425Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11624,"took":"522.308µs","hash":4211733176}
{"level":"info","ts":"2024-02-29T11:23:23.395452Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4211733176,"revision":11624,"compact-revision":11323}
{"level":"info","ts":"2024-02-29T11:28:23.399954Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":11924}
{"level":"info","ts":"2024-02-29T11:28:23.400495Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":11924,"took":"405.313µs","hash":1805113069}
{"level":"info","ts":"2024-02-29T11:28:23.400531Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1805113069,"revision":11924,"compact-revision":11624}
{"level":"info","ts":"2024-02-29T11:33:23.406071Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12227}
{"level":"info","ts":"2024-02-29T11:33:23.406639Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":12227,"took":"405.496µs","hash":428488790}
{"level":"info","ts":"2024-02-29T11:33:23.406673Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":428488790,"revision":12227,"compact-revision":11924}
{"level":"info","ts":"2024-02-29T11:38:23.411086Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12526}
{"level":"info","ts":"2024-02-29T11:38:23.411681Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":12526,"took":"430.759µs","hash":524587330}
{"level":"info","ts":"2024-02-29T11:38:23.411713Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":524587330,"revision":12526,"compact-revision":12227}
{"level":"info","ts":"2024-02-29T11:43:23.414175Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":12827}
{"level":"info","ts":"2024-02-29T11:43:23.414791Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":12827,"took":"410.342µs","hash":679079083}
{"level":"info","ts":"2024-02-29T11:43:23.41482Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":679079083,"revision":12827,"compact-revision":12526}
{"level":"info","ts":"2024-02-29T11:48:23.404563Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13128}
{"level":"info","ts":"2024-02-29T11:48:23.405049Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13128,"took":"355.915µs","hash":2157578418}
{"level":"info","ts":"2024-02-29T11:48:23.405085Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2157578418,"revision":13128,"compact-revision":12827}
{"level":"info","ts":"2024-02-29T11:53:23.416168Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13429}
{"level":"info","ts":"2024-02-29T11:53:23.416849Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13429,"took":"530.338µs","hash":1365778538}
{"level":"info","ts":"2024-02-29T11:53:23.416877Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1365778538,"revision":13429,"compact-revision":13128}
{"level":"info","ts":"2024-02-29T11:58:23.396762Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":13736}
{"level":"info","ts":"2024-02-29T11:58:23.397389Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":13736,"took":"450.55µs","hash":4268939044}
{"level":"info","ts":"2024-02-29T11:58:23.397421Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4268939044,"revision":13736,"compact-revision":13429}
{"level":"info","ts":"2024-02-29T12:03:23.378223Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14036}
{"level":"info","ts":"2024-02-29T12:03:23.378732Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14036,"took":"367.46µs","hash":4121075805}
{"level":"info","ts":"2024-02-29T12:03:23.378762Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4121075805,"revision":14036,"compact-revision":13736}
{"level":"info","ts":"2024-02-29T12:08:23.38123Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14339}
{"level":"info","ts":"2024-02-29T12:08:23.381878Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14339,"took":"497.593µs","hash":2723797132}
{"level":"info","ts":"2024-02-29T12:08:23.381912Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2723797132,"revision":14339,"compact-revision":14036}
{"level":"info","ts":"2024-02-29T12:13:23.391138Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14639}
{"level":"info","ts":"2024-02-29T12:13:23.39164Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14639,"took":"371.714µs","hash":1452563873}
{"level":"info","ts":"2024-02-29T12:13:23.39167Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1452563873,"revision":14639,"compact-revision":14339}
{"level":"info","ts":"2024-02-29T12:18:23.37307Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14942}
{"level":"info","ts":"2024-02-29T12:18:23.373616Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":14942,"took":"402.765µs","hash":2773225297}
{"level":"info","ts":"2024-02-29T12:18:23.373646Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2773225297,"revision":14942,"compact-revision":14639}
{"level":"info","ts":"2024-02-29T12:23:23.376003Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15243}
{"level":"info","ts":"2024-02-29T12:23:23.376477Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15243,"took":"347.815µs","hash":409872866}
{"level":"info","ts":"2024-02-29T12:23:23.376507Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":409872866,"revision":15243,"compact-revision":14942}
{"level":"info","ts":"2024-02-29T12:28:23.381339Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15543}
{"level":"info","ts":"2024-02-29T12:28:23.381902Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15543,"took":"353.22µs","hash":3591615295}
{"level":"info","ts":"2024-02-29T12:28:23.381935Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3591615295,"revision":15543,"compact-revision":15243}
{"level":"info","ts":"2024-02-29T12:33:23.388977Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15845}
{"level":"info","ts":"2024-02-29T12:33:23.389571Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15845,"took":"375.339µs","hash":2500403314}
{"level":"info","ts":"2024-02-29T12:33:23.389603Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2500403314,"revision":15845,"compact-revision":15543}
{"level":"info","ts":"2024-02-29T12:38:23.399768Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16145}
{"level":"info","ts":"2024-02-29T12:38:23.400311Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16145,"took":"397.966µs","hash":310830799}
{"level":"info","ts":"2024-02-29T12:38:23.400359Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":310830799,"revision":16145,"compact-revision":15845}
{"level":"info","ts":"2024-02-29T12:38:43.578217Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-02-29T12:38:43.578262Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-02-29T12:38:43.578369Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-29T12:38:43.578451Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-29T12:38:43.612115Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-29T12:38:43.612385Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-02-29T12:38:43.612415Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-29T12:38:43.615607Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-29T12:38:43.615727Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-29T12:38:43.615744Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [990412c9161b] <==
{"level":"warn","ts":"2024-02-29T12:39:27.828947Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-29T12:39:27.829307Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-02-29T12:39:27.829361Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-02-29T12:39:27.829381Z","caller":"embed/config.go:673","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-02-29T12:39:27.829387Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-29T12:39:27.829423Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-29T12:39:27.829619Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-02-29T12:39:27.829702Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-02-29T12:39:27.830709Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"893.428µs"}
{"level":"info","ts":"2024-02-29T12:39:28.011916Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":10001,"snapshot-size":"8.9 kB"}
{"level":"info","ts":"2024-02-29T12:39:28.011964Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":3358720,"backend-size":"3.4 MB","backend-size-in-use-bytes":1146880,"backend-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-02-29T12:39:28.043126Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":19887}
{"level":"info","ts":"2024-02-29T12:39:28.043573Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-02-29T12:39:28.043633Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 7"}
{"level":"info","ts":"2024-02-29T12:39:28.043661Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 7, commit: 19887, applied: 10001, lastindex: 19887, lastterm: 7]"}
{"level":"info","ts":"2024-02-29T12:39:28.043758Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-29T12:39:28.043782Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-29T12:39:28.04379Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-02-29T12:39:28.044678Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-02-29T12:39:28.045418Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":16145}
{"level":"info","ts":"2024-02-29T12:39:28.04697Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":16467}
{"level":"info","ts":"2024-02-29T12:39:28.048221Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-02-29T12:39:28.049749Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-02-29T12:39:28.049928Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-29T12:39:28.04995Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-29T12:39:28.050105Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-02-29T12:39:28.050166Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-29T12:39:28.050202Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-29T12:39:28.050208Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-29T12:39:28.108923Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-29T12:39:28.10907Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-29T12:39:28.109097Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-29T12:39:28.109207Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-29T12:39:28.109267Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-29T12:39:29.044985Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 7"}
{"level":"info","ts":"2024-02-29T12:39:29.045109Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 7"}
{"level":"info","ts":"2024-02-29T12:39:29.045143Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-02-29T12:39:29.045162Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 8"}
{"level":"info","ts":"2024-02-29T12:39:29.045169Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-29T12:39:29.045181Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 8"}
{"level":"info","ts":"2024-02-29T12:39:29.04519Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-29T12:39:29.049563Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-29T12:39:29.049585Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-29T12:39:29.049643Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-29T12:39:29.050051Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-29T12:39:29.050095Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-29T12:39:29.050512Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-02-29T12:39:29.050515Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-29T12:39:39.022645Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-02-29T12:39:39.025706Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2024-02-29T12:39:39.02578Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":15002}


==> kernel <==
 12:42:35 up  3:47,  0 users,  load average: 0.50, 0.45, 0.34
Linux minikube 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"


==> kindnet [06c7c2f6eadc] <==
I0229 12:37:13.107611       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:37:13.107636       1 main.go:227] handling current node
I0229 12:37:13.107643       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:37:13.107647       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:37:13.107702       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:37:13.107718       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:37:23.111182       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:37:23.111205       1 main.go:227] handling current node
I0229 12:37:23.111211       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:37:23.111214       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:37:23.111264       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:37:23.111266       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:37:33.122545       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:37:33.122605       1 main.go:227] handling current node
I0229 12:37:33.122618       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:37:33.122626       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:37:33.122721       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:37:33.122728       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:37:43.128068       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:37:43.128094       1 main.go:227] handling current node
I0229 12:37:43.128105       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:37:43.128112       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:37:43.128219       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:37:43.128227       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:37:53.131619       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:37:53.131644       1 main.go:227] handling current node
I0229 12:37:53.131651       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:37:53.131654       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:37:53.131703       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:37:53.131731       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:38:03.143677       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:38:03.143702       1 main.go:227] handling current node
I0229 12:38:03.143708       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:38:03.143711       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:38:03.143756       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:38:03.143760       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:38:13.152900       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:38:13.152924       1 main.go:227] handling current node
I0229 12:38:13.152930       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:38:13.152933       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:38:13.152985       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:38:13.152989       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:38:23.156340       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:38:23.156366       1 main.go:227] handling current node
I0229 12:38:23.156372       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:38:23.156376       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:38:23.156422       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:38:23.156426       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:38:33.164371       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:38:33.164395       1 main.go:227] handling current node
I0229 12:38:33.164401       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:38:33.164404       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:38:33.164500       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:38:33.164503       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:38:43.168566       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:38:43.168591       1 main.go:227] handling current node
I0229 12:38:43.168598       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:38:43.168602       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:38:43.168664       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:38:43.168680       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 


==> kindnet [7b077367a449] <==
I0229 12:41:01.458276       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:41:01.458301       1 main.go:227] handling current node
I0229 12:41:01.458307       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:41:01.458310       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:41:01.458379       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:41:01.458395       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:41:11.467482       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:41:11.467526       1 main.go:227] handling current node
I0229 12:41:11.467537       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:41:11.467542       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:41:11.467619       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:41:11.467638       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:41:21.480258       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:41:21.480283       1 main.go:227] handling current node
I0229 12:41:21.480288       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:41:21.480291       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:41:21.480342       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:41:21.480358       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:41:31.484002       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:41:31.484030       1 main.go:227] handling current node
I0229 12:41:31.484037       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:41:31.484041       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:41:31.484108       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:41:31.484112       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:41:41.488191       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:41:41.488232       1 main.go:227] handling current node
I0229 12:41:41.488240       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:41:41.488243       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:41:41.488338       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:41:41.488342       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:41:51.491362       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:41:51.491385       1 main.go:227] handling current node
I0229 12:41:51.491392       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:41:51.491395       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:41:51.491441       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:41:51.491443       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:42:01.495095       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:42:01.495122       1 main.go:227] handling current node
I0229 12:42:01.495128       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:42:01.495131       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:42:01.495181       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:42:01.495183       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:42:11.505291       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:42:11.505319       1 main.go:227] handling current node
I0229 12:42:11.505327       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:42:11.505330       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:42:11.505397       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:42:11.505413       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:42:21.508681       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:42:21.508710       1 main.go:227] handling current node
I0229 12:42:21.508717       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:42:21.508720       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:42:21.508795       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:42:21.508798       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0229 12:42:31.512251       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0229 12:42:31.512278       1 main.go:227] handling current node
I0229 12:42:31.512285       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0229 12:42:31.512289       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0229 12:42:31.512347       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0229 12:42:31.512350       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 


==> kube-apiserver [7711fb501610] <==
I0229 12:39:29.401219       1 handler.go:232] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0229 12:39:29.401239       1 genericapiserver.go:744] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0229 12:39:29.491343       1 handler.go:232] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0229 12:39:29.491400       1 genericapiserver.go:744] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0229 12:39:29.827020       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0229 12:39:29.827054       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0229 12:39:29.827396       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0229 12:39:29.827697       1 secure_serving.go:213] Serving securely on [::]:8443
I0229 12:39:29.827783       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0229 12:39:29.827792       1 available_controller.go:423] Starting AvailableConditionController
I0229 12:39:29.827797       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0229 12:39:29.827806       1 aggregator.go:164] waiting for initial CRD sync...
I0229 12:39:29.827811       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0229 12:39:29.827821       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0229 12:39:29.827858       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0229 12:39:29.827894       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0229 12:39:29.827903       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0229 12:39:29.827930       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0229 12:39:29.827951       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0229 12:39:29.827956       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0229 12:39:29.827990       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0229 12:39:29.828023       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0229 12:39:29.828043       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0229 12:39:29.828037       1 controller.go:116] Starting legacy_token_tracking_controller
I0229 12:39:29.828053       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0229 12:39:29.828120       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0229 12:39:29.828263       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0229 12:39:29.828267       1 controller.go:78] Starting OpenAPI AggregationController
I0229 12:39:29.828296       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0229 12:39:29.828413       1 controller.go:134] Starting OpenAPI controller
I0229 12:39:29.828434       1 controller.go:85] Starting OpenAPI V3 controller
I0229 12:39:29.828447       1 naming_controller.go:291] Starting NamingConditionController
I0229 12:39:29.828456       1 establishing_controller.go:76] Starting EstablishingController
I0229 12:39:29.828466       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0229 12:39:29.828480       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0229 12:39:29.828494       1 crd_finalizer.go:266] Starting CRDFinalizer
I0229 12:39:29.829006       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0229 12:39:29.829169       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0229 12:39:29.911758       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0229 12:39:29.927823       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0229 12:39:29.927896       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0229 12:39:29.927909       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0229 12:39:29.927919       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0229 12:39:29.928030       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0229 12:39:29.928069       1 shared_informer.go:318] Caches are synced for configmaps
I0229 12:39:29.928073       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0229 12:39:29.928088       1 aggregator.go:166] initial CRD sync complete...
I0229 12:39:29.928093       1 autoregister_controller.go:141] Starting autoregister controller
I0229 12:39:29.928095       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0229 12:39:29.928100       1 cache.go:39] Caches are synced for autoregister controller
E0229 12:39:29.931674       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0229 12:39:29.941458       1 shared_informer.go:318] Caches are synced for node_authorizer
I0229 12:39:30.831139       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0229 12:39:31.791942       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0229 12:39:31.856918       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0229 12:39:31.861707       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0229 12:39:31.893472       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0229 12:39:31.898101       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0229 12:39:41.953885       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0229 12:39:42.338400       1 controller.go:624] quota admission added evaluator for: endpoints


==> kube-apiserver [7ac9084a2ca9] <==
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0229 12:38:53.361955       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0229 12:38:53.362094       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0229 12:38:53.368907       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0229 12:38:53.408440       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0229 12:38:53.410817       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0229 12:38:53.460962       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0229 12:38:53.544988       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [803b1c519357] <==
I0229 12:39:42.565466       1 shared_informer.go:318] Caches are synced for garbage collector
I0229 12:39:42.565492       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0229 12:39:42.760806       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="52.195µs"
I0229 12:39:42.766283       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="57.08µs"
I0229 12:39:42.772102       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="47.687µs"
I0229 12:39:42.777310       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="50.089µs"
I0229 12:39:42.903219       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="69.099µs"
I0229 12:39:42.904311       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="41.032µs"
I0229 12:39:42.908785       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="25.829µs"
I0229 12:39:42.912805       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="21.442µs"
I0229 12:39:47.829033       1 topologycache.go:237] "Can't get CPU or zone information for node" node="minikube-m03"
I0229 12:39:47.829092       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m02\" does not exist"
I0229 12:39:47.833655       1 range_allocator.go:380] "Set node PodCIDR" node="minikube-m02" podCIDRs=["10.244.1.0/24"]
I0229 12:39:48.180105       1 topologycache.go:237] "Can't get CPU or zone information for node" node="minikube-m03"
I0229 12:39:51.997751       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube-m02"
I0229 12:39:51.997767       1 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m02 event: Registered Node minikube-m02 in Controller"
I0229 12:39:54.702748       1 event.go:307] "Event occurred" object="default/postgres-dc95b6f47" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-dc95b6f47-g8p49"
I0229 12:39:54.705237       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-7fd5cb4ddc-jr8ln"
I0229 12:39:54.706619       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="8.00025ms"
I0229 12:39:54.710362       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="8.120151ms"
I0229 12:39:54.712198       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="5.540653ms"
I0229 12:39:54.712198       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-8694d4445c-z9bk8"
I0229 12:39:54.712263       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="29.209µs"
I0229 12:39:54.714745       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="4.334464ms"
I0229 12:39:54.714821       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="28.863µs"
I0229 12:39:54.717804       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="11.167757ms"
I0229 12:39:54.718445       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="22.543µs"
I0229 12:39:54.721783       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="3.925974ms"
I0229 12:39:54.721857       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="36.315µs"
I0229 12:39:54.721918       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="47.155µs"
I0229 12:39:54.724529       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="28.785µs"
I0229 12:39:54.731109       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="55.588µs"
I0229 12:39:55.803633       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="4.463197ms"
I0229 12:39:55.803717       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="35.144µs"
I0229 12:39:55.809073       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="4.23392ms"
I0229 12:39:55.809132       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="24.499µs"
I0229 12:39:57.731827       1 topologycache.go:237] "Can't get CPU or zone information for node" node="minikube-m02"
I0229 12:39:58.394370       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m03\" does not exist"
I0229 12:39:58.394418       1 topologycache.go:237] "Can't get CPU or zone information for node" node="minikube-m02"
I0229 12:39:58.398108       1 range_allocator.go:380] "Set node PodCIDR" node="minikube-m03" podCIDRs=["10.244.2.0/24"]
I0229 12:39:58.747700       1 topologycache.go:237] "Can't get CPU or zone information for node" node="minikube-m03"
I0229 12:39:59.297320       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="39.893µs"
I0229 12:39:59.304240       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="44.541µs"
I0229 12:39:59.309807       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="42.805µs"
I0229 12:39:59.321287       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="31.457µs"
I0229 12:39:59.324532       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="32.072µs"
I0229 12:39:59.325528       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="21.715µs"
I0229 12:39:59.330749       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="29.55µs"
I0229 12:39:59.333882       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="26.123µs"
I0229 12:39:59.335081       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="24.584µs"
I0229 12:39:59.613624       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="34.325µs"
I0229 12:39:59.616680       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="24.487µs"
I0229 12:40:06.851677       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="60.096µs"
I0229 12:40:20.740191       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="59.416µs"
I0229 12:40:46.742042       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="54.588µs"
I0229 12:40:58.744474       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="57.262µs"
I0229 12:41:22.742547       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="58.677µs"
I0229 12:41:34.746311       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="55.545µs"
I0229 12:42:11.742208       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="62.3µs"
I0229 12:42:26.748027       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="50.093µs"


==> kube-controller-manager [ffeec317e347] <==
I0229 09:17:53.345683       1 event.go:307] "Event occurred" object="minikube-m02" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m02 event: Registered Node minikube-m02 in Controller"
I0229 09:18:17.637392       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m03\" does not exist"
I0229 09:18:17.637441       1 topologycache.go:237] "Can't get CPU or zone information for node" node="minikube-m02"
I0229 09:18:17.643729       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-lmbbh"
I0229 09:18:17.643756       1 event.go:307] "Event occurred" object="kube-system/kindnet" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kindnet-9ndrz"
I0229 09:18:17.647356       1 range_allocator.go:380] "Set node PodCIDR" node="minikube-m03" podCIDRs=["10.244.2.0/24"]
I0229 09:18:17.881784       1 topologycache.go:237] "Can't get CPU or zone information for node" node="minikube-m02"
I0229 09:18:18.348696       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube-m03"
I0229 09:18:18.348728       1 event.go:307] "Event occurred" object="minikube-m03" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube-m03 event: Registered Node minikube-m03 in Controller"
I0229 09:20:39.587860       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set dashboard-metrics-scraper-7fd5cb4ddc to 1"
I0229 09:20:39.589019       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set kubernetes-dashboard-8694d4445c to 1"
I0229 09:20:39.599067       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"dashboard-metrics-scraper-7fd5cb4ddc-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0229 09:20:39.599097       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Warning" reason="FailedCreate" message="Error creating: pods \"kubernetes-dashboard-8694d4445c-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found"
I0229 09:20:39.602382       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="13.398051ms"
E0229 09:20:39.602418       1 replica_set.go:557] sync "kubernetes-dashboard/kubernetes-dashboard-8694d4445c" failed with pods "kubernetes-dashboard-8694d4445c-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0229 09:20:39.602393       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="14.627062ms"
E0229 09:20:39.602435       1 replica_set.go:557] sync "kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" failed with pods "dashboard-metrics-scraper-7fd5cb4ddc-" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount "kubernetes-dashboard" not found
I0229 09:20:39.608272       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: dashboard-metrics-scraper-7fd5cb4ddc-fq2vx"
I0229 09:20:39.610876       1 event.go:307] "Event occurred" object="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kubernetes-dashboard-8694d4445c-t2jn6"
I0229 09:20:39.612839       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="8.654365ms"
I0229 09:20:39.614888       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="12.4386ms"
I0229 09:20:39.617954       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="5.076691ms"
I0229 09:20:39.618013       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="28.03µs"
I0229 09:20:39.619585       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="4.648405ms"
I0229 09:20:39.619662       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="22.305µs"
I0229 09:20:39.620878       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="24.863µs"
I0229 09:20:39.623969       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="40.243µs"
I0229 09:21:00.358704       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="3.83519ms"
I0229 09:21:00.358782       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="26.846µs"
I0229 09:21:26.181977       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="4.899749ms"
I0229 09:21:26.182045       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="37.19µs"
I0229 10:28:25.139098       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-s2zrc" approvedExpiration="1h0m0s"
I0229 10:28:25.153424       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-8nbtm" approvedExpiration="1h0m0s"
I0229 10:28:25.155867       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-8nbs6" approvedExpiration="1h0m0s"
I0229 10:49:14.756671       1 event.go:307] "Event occurred" object="default/postgres" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres-55c7db8f7 to 1"
I0229 10:49:14.767569       1 event.go:307] "Event occurred" object="default/postgres-55c7db8f7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-55c7db8f7-kbdw9"
I0229 10:49:14.772808       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="15.855726ms"
I0229 10:49:14.776814       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="3.961614ms"
I0229 10:49:14.783928       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="7.076744ms"
I0229 10:49:14.784248       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="231.653µs"
I0229 10:50:28.743135       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="5.293086ms"
I0229 10:50:28.743254       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="68.506µs"
I0229 11:11:55.470012       1 event.go:307] "Event occurred" object="default/django-postgres-pvc" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0229 11:11:55.470079       1 event.go:307] "Event occurred" object="default/django-postgres-pvc" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="ExternalProvisioning" message="Waiting for a volume to be created either by the external provisioner 'k8s.io/minikube-hostpath' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered."
I0229 11:11:55.501754       1 event.go:307] "Event occurred" object="default/postgres" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set postgres-55c7db8f7 to 0 from 1"
I0229 11:11:55.506656       1 event.go:307] "Event occurred" object="default/postgres-55c7db8f7" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: postgres-55c7db8f7-kbdw9"
I0229 11:11:55.513536       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="11.869084ms"
I0229 11:11:55.516530       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="2.945075ms"
I0229 11:11:55.516712       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="106.454µs"
I0229 11:11:55.704741       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="50.704µs"
I0229 11:11:55.711530       1 event.go:307] "Event occurred" object="default/postgres" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set postgres-dc95b6f47 to 1"
I0229 11:11:55.714938       1 event.go:307] "Event occurred" object="default/postgres-dc95b6f47" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: postgres-dc95b6f47-xq2kd"
I0229 11:11:55.720334       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="8.897151ms"
I0229 11:11:55.729567       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="9.179626ms"
I0229 11:11:55.729646       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="30.889µs"
I0229 11:11:56.009096       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="80.074µs"
I0229 11:11:56.017767       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="36.246µs"
I0229 11:11:56.026248       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-55c7db8f7" duration="68.404µs"
I0229 11:11:56.982033       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="5.514707ms"
I0229 11:11:56.982134       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/postgres-dc95b6f47" duration="38.442µs"


==> kube-proxy [695dde55572e] <==
I0229 09:16:19.262037       1 server_others.go:69] "Using iptables proxy"
E0229 09:16:19.263379       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I0229 09:16:21.995268       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0229 09:16:22.105533       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0229 09:16:22.106719       1 server_others.go:152] "Using iptables Proxier"
I0229 09:16:22.106747       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0229 09:16:22.106752       1 server_others.go:438] "Defaulting to no-op detect-local"
I0229 09:16:22.106788       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0229 09:16:22.106954       1 server.go:846] "Version info" version="v1.28.3"
I0229 09:16:22.106972       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0229 09:16:22.107353       1 config.go:188] "Starting service config controller"
I0229 09:16:22.107386       1 shared_informer.go:311] Waiting for caches to sync for service config
I0229 09:16:22.107425       1 config.go:97] "Starting endpoint slice config controller"
I0229 09:16:22.107446       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0229 09:16:22.107555       1 config.go:315] "Starting node config controller"
I0229 09:16:22.107586       1 shared_informer.go:311] Waiting for caches to sync for node config
I0229 09:16:22.207870       1 shared_informer.go:318] Caches are synced for node config
I0229 09:16:22.207887       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0229 09:16:22.207935       1 shared_informer.go:318] Caches are synced for service config


==> kube-proxy [86f92ab350d7] <==
I0229 12:39:30.640920       1 server_others.go:69] "Using iptables proxy"
I0229 12:39:30.709292       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0229 12:39:30.723973       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0229 12:39:30.725176       1 server_others.go:152] "Using iptables Proxier"
I0229 12:39:30.725212       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0229 12:39:30.725218       1 server_others.go:438] "Defaulting to no-op detect-local"
I0229 12:39:30.725252       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0229 12:39:30.725404       1 server.go:846] "Version info" version="v1.28.3"
I0229 12:39:30.725423       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0229 12:39:30.725802       1 config.go:188] "Starting service config controller"
I0229 12:39:30.725835       1 shared_informer.go:311] Waiting for caches to sync for service config
I0229 12:39:30.725815       1 config.go:97] "Starting endpoint slice config controller"
I0229 12:39:30.725874       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0229 12:39:30.727125       1 config.go:315] "Starting node config controller"
I0229 12:39:30.727174       1 shared_informer.go:311] Waiting for caches to sync for node config
I0229 12:39:30.826334       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0229 12:39:30.826371       1 shared_informer.go:318] Caches are synced for service config
I0229 12:39:30.827246       1 shared_informer.go:318] Caches are synced for node config


==> kube-scheduler [0cac218d4961] <==
I0229 12:39:28.347352       1 serving.go:348] Generated self-signed cert in-memory
W0229 12:39:29.837794       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0229 12:39:29.837828       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0229 12:39:29.837836       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0229 12:39:29.837842       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0229 12:39:29.925314       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0229 12:39:29.925340       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0229 12:39:29.926166       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0229 12:39:29.926211       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0229 12:39:29.926590       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0229 12:39:29.926615       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0229 12:39:30.026778       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [9b30abcf576c] <==
I0229 09:16:19.835252       1 serving.go:348] Generated self-signed cert in-memory
W0229 09:16:21.989940       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0229 09:16:21.989978       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0229 09:16:21.989985       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0229 09:16:21.989988       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0229 09:16:22.090108       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0229 09:16:22.090135       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0229 09:16:22.091508       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0229 09:16:22.091558       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0229 09:16:22.092044       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0229 09:16:22.092082       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0229 09:16:22.192079       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0229 12:38:43.577442       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0229 12:38:43.577467       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0229 12:38:43.577611       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0229 12:38:43.578245       1 run.go:74] "command failed" err="finished without leader elect"


==> kubelet <==
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.114356    1696 topology_manager.go:215] "Topology Admit Handler" podUID="6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd" podNamespace="kube-system" podName="kindnet-2tprc"
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.212191    1696 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.244517    1696 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/f0d52988-4ba9-4379-a912-25c770957a72-lib-modules\") pod \"kube-proxy-qrhwp\" (UID: \"f0d52988-4ba9-4379-a912-25c770957a72\") " pod="kube-system/kube-proxy-qrhwp"
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.244598    1696 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd-lib-modules\") pod \"kindnet-2tprc\" (UID: \"6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd\") " pod="kube-system/kindnet-2tprc"
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.244726    1696 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/a251b6d5-f129-4a41-9eea-19f274e36ba4-tmp\") pod \"storage-provisioner\" (UID: \"a251b6d5-f129-4a41-9eea-19f274e36ba4\") " pod="kube-system/storage-provisioner"
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.244760    1696 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd-cni-cfg\") pod \"kindnet-2tprc\" (UID: \"6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd\") " pod="kube-system/kindnet-2tprc"
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.244791    1696 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/f0d52988-4ba9-4379-a912-25c770957a72-xtables-lock\") pod \"kube-proxy-qrhwp\" (UID: \"f0d52988-4ba9-4379-a912-25c770957a72\") " pod="kube-system/kube-proxy-qrhwp"
Feb 29 12:39:30 minikube kubelet[1696]: I0229 12:39:30.244832    1696 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd-xtables-lock\") pod \"kindnet-2tprc\" (UID: \"6f36b9b6-f0ff-4281-a5f8-d3d3151b10fd\") " pod="kube-system/kindnet-2tprc"
Feb 29 12:39:30 minikube kubelet[1696]: E0229 12:39:30.343798    1696 kubelet.go:1890] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Feb 29 12:39:32 minikube kubelet[1696]: I0229 12:39:32.373647    1696 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 29 12:39:37 minikube kubelet[1696]: E0229 12:39:37.312862    1696 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 29 12:39:37 minikube kubelet[1696]: E0229 12:39:37.312904    1696 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 29 12:39:39 minikube kubelet[1696]: I0229 12:39:39.140096    1696 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 29 12:39:41 minikube kubelet[1696]: I0229 12:39:41.432705    1696 scope.go:117] "RemoveContainer" containerID="dda3a41a93f3510fd06e0f31f745a90981e5a4032d6a2cdd2ca283d973c4eb5a"
Feb 29 12:39:41 minikube kubelet[1696]: I0229 12:39:41.432896    1696 scope.go:117] "RemoveContainer" containerID="350d35d053fe4c8da5b09ccb03c6434400f1965f23804614e672b395d4add327"
Feb 29 12:39:41 minikube kubelet[1696]: E0229 12:39:41.433074    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(a251b6d5-f129-4a41-9eea-19f274e36ba4)\"" pod="kube-system/storage-provisioner" podUID="a251b6d5-f129-4a41-9eea-19f274e36ba4"
Feb 29 12:39:42 minikube kubelet[1696]: E0229 12:39:42.035827    1696 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:39:42 minikube kubelet[1696]: E0229 12:39:42.035882    1696 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:39:42 minikube kubelet[1696]: E0229 12:39:42.036010    1696 kuberuntime_manager.go:1256] container &Container{Name:nginx,Image:nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9x25h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592): ErrImagePull: Error response from daemon: Head "https://registry-1.docker.io/v2/library/nginx/manifests/latest": Get "https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io": net/http: TLS handshake timeout
Feb 29 12:39:42 minikube kubelet[1696]: E0229 12:39:42.036053    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with ErrImagePull: \"Error response from daemon: Head \\\"https://registry-1.docker.io/v2/library/nginx/manifests/latest\\\": Get \\\"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\\\": net/http: TLS handshake timeout\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:39:42 minikube kubelet[1696]: I0229 12:39:42.441784    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:39:42 minikube kubelet[1696]: E0229 12:39:42.442877    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"nginx\\\"\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:39:47 minikube kubelet[1696]: E0229 12:39:47.326612    1696 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 29 12:39:47 minikube kubelet[1696]: E0229 12:39:47.326652    1696 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 29 12:39:54 minikube kubelet[1696]: I0229 12:39:54.107983    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:39:54 minikube kubelet[1696]: I0229 12:39:54.108075    1696 scope.go:117] "RemoveContainer" containerID="350d35d053fe4c8da5b09ccb03c6434400f1965f23804614e672b395d4add327"
Feb 29 12:39:57 minikube kubelet[1696]: E0229 12:39:57.394789    1696 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 29 12:39:57 minikube kubelet[1696]: E0229 12:39:57.394825    1696 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 29 12:40:04 minikube kubelet[1696]: E0229 12:40:04.759377    1696 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:40:04 minikube kubelet[1696]: E0229 12:40:04.759423    1696 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:40:04 minikube kubelet[1696]: E0229 12:40:04.759572    1696 kuberuntime_manager.go:1256] container &Container{Name:nginx,Image:nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9x25h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592): ErrImagePull: Error response from daemon: Head "https://registry-1.docker.io/v2/library/nginx/manifests/latest": Get "https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io": net/http: TLS handshake timeout
Feb 29 12:40:04 minikube kubelet[1696]: E0229 12:40:04.759603    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with ErrImagePull: \"Error response from daemon: Head \\\"https://registry-1.docker.io/v2/library/nginx/manifests/latest\\\": Get \\\"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\\\": net/http: TLS handshake timeout\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:40:07 minikube kubelet[1696]: E0229 12:40:07.498243    1696 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 29 12:40:07 minikube kubelet[1696]: E0229 12:40:07.498298    1696 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 29 12:40:17 minikube kubelet[1696]: E0229 12:40:17.528017    1696 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Feb 29 12:40:17 minikube kubelet[1696]: E0229 12:40:17.528055    1696 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Feb 29 12:40:19 minikube kubelet[1696]: I0229 12:40:19.105383    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:40:19 minikube kubelet[1696]: E0229 12:40:19.107734    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with ImagePullBackOff: \"Back-off pulling image \\\"nginx\\\"\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:40:30 minikube kubelet[1696]: I0229 12:40:30.105341    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:40:40 minikube kubelet[1696]: E0229 12:40:40.806230    1696 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:40:40 minikube kubelet[1696]: E0229 12:40:40.806268    1696 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Head \"https://registry-1.docker.io/v2/library/nginx/manifests/latest\": Get \"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:40:40 minikube kubelet[1696]: E0229 12:40:40.806331    1696 kuberuntime_manager.go:1256] container &Container{Name:nginx,Image:nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9x25h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592): ErrImagePull: Error response from daemon: Head "https://registry-1.docker.io/v2/library/nginx/manifests/latest": Get "https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io": net/http: TLS handshake timeout
Feb 29 12:40:40 minikube kubelet[1696]: E0229 12:40:40.806360    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with ErrImagePull: \"Error response from daemon: Head \\\"https://registry-1.docker.io/v2/library/nginx/manifests/latest\\\": Get \\\"https://auth.docker.io/token?scope=repository%!A(MISSING)library%!F(MISSING)nginx%!A(MISSING)pull&service=registry.docker.io\\\": net/http: TLS handshake timeout\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:40:52 minikube kubelet[1696]: I0229 12:40:52.106445    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:40:52 minikube kubelet[1696]: E0229 12:40:52.106622    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=nginx pod=nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592)\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:41:07 minikube kubelet[1696]: I0229 12:41:07.106250    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:41:07 minikube kubelet[1696]: E0229 12:41:07.106379    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=nginx pod=nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592)\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:41:22 minikube kubelet[1696]: I0229 12:41:22.107570    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:41:22 minikube kubelet[1696]: E0229 12:41:22.107749    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=nginx pod=nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592)\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:41:33 minikube kubelet[1696]: I0229 12:41:33.107966    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:41:43 minikube kubelet[1696]: E0229 12:41:43.123472    1696 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:41:43 minikube kubelet[1696]: E0229 12:41:43.123525    1696 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: Get \"https://registry-1.docker.io/v2/\": net/http: TLS handshake timeout" image="nginx:latest"
Feb 29 12:41:43 minikube kubelet[1696]: E0229 12:41:43.123613    1696 kuberuntime_manager.go:1256] container &Container{Name:nginx,Image:nginx,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9x25h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592): ErrImagePull: Error response from daemon: Get "https://registry-1.docker.io/v2/": net/http: TLS handshake timeout
Feb 29 12:41:43 minikube kubelet[1696]: E0229 12:41:43.123650    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with ErrImagePull: \"Error response from daemon: Get \\\"https://registry-1.docker.io/v2/\\\": net/http: TLS handshake timeout\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:41:55 minikube kubelet[1696]: I0229 12:41:55.108007    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:41:55 minikube kubelet[1696]: E0229 12:41:55.108161    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=nginx pod=nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592)\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:42:08 minikube kubelet[1696]: I0229 12:42:08.108075    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:42:08 minikube kubelet[1696]: E0229 12:42:08.108235    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=nginx pod=nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592)\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"
Feb 29 12:42:22 minikube kubelet[1696]: I0229 12:42:22.107713    1696 scope.go:117] "RemoveContainer" containerID="fcd97146e9b573d5df5ee01fb1816d8bdf1d2583cd36e9ea43d2314bd7cae47c"
Feb 29 12:42:22 minikube kubelet[1696]: E0229 12:42:22.107907    1696 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"nginx\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=nginx pod=nginx_default(535ae7ce-48d6-4d4a-a928-0c49625ad592)\"" pod="default/nginx" podUID="535ae7ce-48d6-4d4a-a928-0c49625ad592"

